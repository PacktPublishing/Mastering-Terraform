# 2. Cloud Architecture

Google Kubernetes Engine is a sophisticated offering that allows you to provision a managed Kubernetes cluster in a multitude of ways depending on your objectives. Whether that is to maximize simplicity of operations or highly customized configurations.

## Autopilot

One of the simplest ways of operating a Kubernetes cluster on Google Cloud is using the Autopilot feature of Google Kubernetes Engine. Turning on the Autopilot feature abstracts much of the complexity of operating a Kubernetes cluster. This option changes the operating model very radically, so much so that it is probably more akin to some of the container-based server-less options on other clouds than it does the managed Kubernetes offerings that we’ve delved into in previous chapters. As a result, it is out-of-scope for this book. However, if this approach appeals to you, I suggest you investigate further in Google’s own documentation.[^1] I point this out because, unlike AWS and Azure, which have separately branded services that abstract way the container orchestration, Google Cloud Platform has this capability coupled with its managed Kubernetes offering.

## Regional vs. Zonal

Google Kubernetes Engine supports two primary cluster types: Regional and Zonal. The cluster type affects how the cluster’s underlying physical infrastructure is provisioned across the Google Cloud Platform which subsequently affects the resiliency of the Kubernetes cluster. 

![Resource][image-2]

_GKE Zonal Cluster hosts the control plane and all nodes within a single Availability Zone_

A Zonal Cluster is deployed within a single Availability Zone within a given region. As we know, each region has a name `us-west1` and to reference a specific zone we append the zone number to the end of the region name. For example, to reference Availability Zone A in the West US 1 region, we can refer to it by its name: `us-west1-a`. 

![Resource][image-1]

_GKE Regional Cluster replicates the control plane and nodes across all zones within the region_

A Regional Cluster is deployed across Availability Zones within a given region. When you deploy a Regional Cluster, by default, your cluster is deployed across three Availability Zones within that region. This approach results in higher availability and resiliency in case of an Availability Zone outage. 

## Virtual Network

As we discussed in the previous chapter, when we setup our Virtual Machine based solution on Google Cloud, we will need a Virtual Network to host our Google Kubernetes Engine cluster. This will allow us to configure a private Google Kubernetes Engine cluster such that the Kubernetes control plane and the node have private IP addresses and are not directly accessible from the Internet.

In the previous chapter, where we set up our Virtual Machine based solution, we set up two subnets: one for the front end and one for the back end. However, when using a Kubernetes cluster to host our solution, both the front and back end will be hosted on the same Kubernetes nodes. 

This straightforward approach, where multiple node pools share one subnet, can suffice for less complex configurations. However, while this setup simplifies network management, it can potentially limit the scalability of individual node pools due to shared network resources and address space constraints. 

For more scalable and flexible architectures, especially in larger or more dynamic environments, it's often advantageous to allocate separate subnets for different node pools. This method allows each node pool to scale independently and optimizes network organization, providing better resource allocation and isolation. This kind of structured subnetting becomes increasingly important as the complexity and scale of the Kubernetes deployments grow, making it a key consideration in GKE network planning and configuration.

## Container Registry

Like the other cloud platforms we’ve been delving into in this book, Google Cloud also offers a robust Container Registry service, known as Google Artifact Registry, which is a private registry for hosting and managing container images and Helm charts. The Artifact Registry supports many other formats besides container images but we'll only be using it in this capacity.

Google Artifact Registry is setup pretty similarly to other cloud providers. It resembles the Azure Container Registry a bit more though because it can host multiple repositories allowing you to host multiple container images in the same Artifact Registry. 

## Load Balancing

Google Kubernetes Engine has a very similar experience to other managed Kubernetes offerings that we have looked at in this book. By default, when a Kubernetes service is provisioned to a private cluster, GKE will automatically provision an Internal Load Balancer for this service. This will make the Kubernetes service available within the Virtual Network but not to the outside world.

This works well for our backend REST API but doesn’t work for our public web application, which is intended to be accessible from the public Internet. Like on AWS and Azure, in order to make the frontend service accessible to the Internet, we need to configure an Ingress Controller on the cluster and a public load balancer that has a public IP address and would route traffic to the ingress controller on the GKE cluster. 

![Resource][image-3]

_GKE Cluster with NGINX ingress controller automating Google Cloud Load Balancer_

As we did in previous chapters, we'll setup an NGINX ingress controller and configure it to automatically provision the necessary external load balancer.

## Network Security

When working with Google Kubernetes Engine (GKE), network security is managed in a manner akin to the practices described in Chapter 13 for Virtual Machines, leveraging similar concepts and tools within the Google Cloud ecosystem. GKE clusters are typically deployed within a Virtual Network, allowing them to seamlessly integrate with other Google Cloud services.

Similar to the other managed Kubernetes offerings, the Virtual Network acts as the primary boundary for network security, within which GKE has its internal network where pods and services communicate. Google Cloud Firewalls are used to define security rules at the subnet level, controlling inbound and outbound traffic similar to how they are employed with Virtual Machines. 

Additionally, GKE takes advantage of native Kubernetes Network Policies for finer-grained control within the cluster, allowing administrators to define how pods communicate with each other and with other resources in the Virtual Network. This dual-layered approach, combining the external security controls of the Virtual Network with the internal mechanisms of GKE, creates a comprehensive and robust network security environment for Kubernetes deployments.

## Workload Identity

As we did with AWS and Azure in previous chapters, we'll be setting up Workload Identity to allow our application's pods to authenticate with other Google Cloud services using Google Cloud identity provider. This will allow us to use the built-in role-based access control to grant access for Kubernetes service accounts to other Google Cloud resources.

## Secrets Management

Google Kubernetes Engine does not have direct integration with Google Secrets Manager like other cloud platforms. Instead, the options available to you are to leverage native Kubernetes secrets or to access Google Secrets Manager from your application code itself. This approach does have some security advantages but it is less ideal as it tightly couples your application to Google Cloud Platform SDKs.

## Kubernetes Cluster
Building a Kubernetes cluster using Google Kubernetes Engine (GKE) involves a few key decisions that determine the modality of your cluster. As we’ve discussed, in this book, we will omit the use of Autopilot in order to maintain congruency with the other managed Kubernetes offerings from the other cloud platforms we’ve looked at in this book, and we will focus on building a private Kubernetes cluster with its own Virtual Network.

Like other managed Kubernetes offerings, GKE provides flexibility to configure node pools based on workload types, but unlike those offerings, you don’t need to set up node pools for running core Kubernetes services. GKE handles all that on your behalf! This abstraction greatly simplifies cluster design. Overall, GKE’s simplicity and robust feature set allow us to build highly scalable Kubernetes clusters with minimal effort.

[^1]:	Google Cloud Platform Documentation: Autopilot Overview: [https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview]


[image-1]:	../images/GCP-Cluster-Regional.png
[image-2]:	../images/GCP-Cluster-Zonal.png
[image-3]:	../images/GCP-GKE-NetworkFlow.png