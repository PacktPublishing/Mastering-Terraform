# 3. Infrastructure

As we know, Terraform is not a write-once, run-anywhere solution. It is a highly extensible Infrastructure-as-Code tool that uses a well-defined strategy pattern to facilitate the management of multiple cloud platforms. This yields very similar conceptually structured solutions but with significant variations embedded within the differing implementation details and nomenclature of each corresponding cloud platform.

As we discussed in the previous section, the virtual network configuration will largely be identical and the load balancer will be automatically provisioned by GKE via the NGINX ingress controller. Therefore, in this section we will only focus on the new resources that we need in order to replace our virtual machines with a Kubernetes cluster.

## Container Registry

The first thing we need is a Google Cloud Artifact Registry that we can push docker images to. We'll use this as part of our Docker build process later when we build and push docker images to be used by our GKE cluster.

```
resource "google_artifact_registry_repository" "main" {
  project       = google_project.main.project_id
  location      = var.primary_region
  repository_id = "${var.application_name}-${var.environment_name}"
  format        = "DOCKER"
}
```

## Service Account

In order to grant our applications and services the ability to implicitly authenticate with Google Cloud and access other services and resources hosted therein, we need to set up a service account that we can associate with the workloads running on our cluster. This is similar to the IAM Role and Managed Identity we specified on AWS and Azure, respectively.

```
resource "google_service_account" "cluster" {
  project      = google_project.main.project_id
  account_id   = "sa-gke-${var.application_name}-${var.environment_name}-${random_string.project_id.result}"
  display_name = "sa-gke-${var.application_name}-${var.environment_name}-${random_string.project_id.result}"
}
```

## Kubernetes Cluster

This Terraform code creates an Google Kubernetes Engine (GKE) cluster with a customized name, Google Cloud Region. The `location` attribute is extremely critical as its value can determine if the cluster is a Regional or Zonal cluster. Simply by making a tiny change from `us-west1` to `us-west1-a` creates this affect.

```
resource "google_container_cluster" "main" {

  project  = google_project.main.project_id
  name     = "gke-${var.application_name}-${var.environment_name}-${random_string.project_id.result}"
  location = var.primary_region
	
  remove_default_node_pool = true
  initial_node_count       = 1
	  
}
```

By default Google Kubernetes Engine will automatically provision a default node pool. This is a common practice that, unfortunately, prioritizes the graphical user experience via the Google Cloud Console over the Infrastructure-as-Code experience. This problem is not unique to Google Cloud; both AWS and Azure have similar areas of friction where automation is an afterthought. As a result, we are at least left with attributes that allow us to circumvent this behavior. By setting the `remove_default_node_pool` to `true`, we can ensure that this default behavior is eliminated. Further setting the `initial_node_count` to `1` can further speed up this process.

As we discussed previously, GKE abstracts the Kubernetes master services from us so that we donâ€™t need to worry about deploying a node pool for these Kubernetes system components. Therefore, we are left with defining just our own node pools for our applications and services to run on. 

```
resource "google_container_node_pool" "primary" {

  project    = google_project.main.project_id
  name       = "gke-${var.application_name}-${var.environment_name}-${random_string.project_id.result}-primary"
  location   = var.primary_region
  cluster    = google_container_cluster.main.name
  node_count = var.node_count

  node_config {

    ...
	
  }

}
```

The basic configuration of a node pool resource connects it to the corresponding cluster and specifies a `node_count`. The `node_config` block is where we configure more details for the nodes within the pool. The node pool configuration should look similar to what we saw in Chapters 8 and 11 when configuring the managed Kubernetes offerings of AWS and Azure. Node pools have a count that controls how many Virtual Machines we spin up and a Virtual Machine Size that specifies how many CPU cores and memory each node gets. We also need to specify the service account under which the node pool will operate.

```
node_config {

  machine_type = var.node_size

  preemptible  = false
  spot         = false

  service_account = google_service_account.cluster.email
    
  oauth_scopes = [
    "https://www.googleapis.com/auth/cloud-platform",
    "https://www.googleapis.com/auth/logging.write",
    "https://www.googleapis.com/auth/monitoring"
  ]
}
```

The `oauth_scopes` is used to specify what permissions the nodes should have access to. In order to enable Google Cloud logging and monitoring we need to add scopes to allow the nodes to tap into these existing Google Cloud services.

## Workload Identity

In order to enable Workload Identity we need to modify both our cluster and node pool configuration. The cluster needs to have the `workload_identity_config` block defined with a `workload_pool` set with a specific magic string that will provision the GKE meta-data service within the cluster.

```
resource "google_container_cluster" "main" {

  ...

  workload_identity_config {
    workload_pool = "${google_project.main.project_id}.svc.id.goog"
  }

}
```

Once the GKE meta-data service is made available within the cluster, we need to configure our node pools to integrate with it using the `workload_metadata_config` block by specifying `GKE_METADATA` as the mode.

```
node_config {

  ...

  workload_metadata_config {
    mode = "GKE_METADATA"
  }

}
```