# 2. Terraform

As we discussed in our design, our solution comprises two Application Components: the front end and the back end. Each has its own application codebase that needs to be deployed. Unlike previous chapters, where we also had Operating System configuration, now that we are using Serverless offerings, this is no longer our responsibility as the platform takes care of it for us.

![Resource][image-1]
_Google Cloud Function resource structure_

Much of the Terraform setup is very similar to what we have done in previous chapters so we will only focus on new resources needed for our solution. You can check the full source code for this book which is available on GitHub if you want to work with the complete solution.

## Frontend

As we saw in previous chapters, when working with Google Cloud, we need to activate the required Google APIs to provision resources to our new project. For the front end, we will mainly use Google Cloud Storage, but we also need a Cloud Load Balancer, which requires the `compute.googleapis.com` API.

First, we need to provision a Google Cloud Storage Bucket to which we can deploy our Frontend. However, we need to configure our Google Cloud Storage Bucket differently using an optional block called `website` to enable the static websites feature.

	resource "google_storage_bucket" "frontend" {
	
	  project  = google_project.main.project_id
	  name     = "${var.application_name}-${var.environment_name}-frontend-${random_string.project_id.result}"
	  location = "US"
	
	  website {
	    main_page_suffix = "index.html"
	    not_found_page   = "404.html"
	  }
	
	  cors {
	    origin          = ["*"]
	    method          = ["GET", "HEAD"]
	    response_header = ["Authorization", "Content-Type"]
	    max_age_seconds = 3600
	  }
	
	}

To allow anonymous internet traffic to access the content stored within the bucket, we need to set up a binding with the Identity & Access Management service. This will grant `allUsers` access to view objects within the storage bucket.

	resource "google_storage_bucket_iam_binding" "frontend" {
	  bucket = google_storage_bucket.frontend.name
	  role   = "roles/storage.objectViewer"
	
	  members = [
	    "allUsers"
	  ]
	}

In previous chapters, we’ve set up Google Cloud Load Balancers, which establishes a load balancer as the frontend and allows you to configure many different types of backends.


![Resource][image-2]
_Google Cloud Load Balancer routes traffic to the Front end hosted on Google Cloud Storage_

In this case, the backend for the load balancer becomes extremely simple, it's just a Cloud Storage Bucket.

	resource "google_compute_backend_bucket" "frontend" {
	
	  project = google_project.main.project_id
	
	  name        = "${var.application_name}-${var.environment_name}-frontend-${random_string.project_id.result}"
	  bucket_name = google_storage_bucket.frontend.name
	  enable_cdn  = true
	
	}

The Google Cloud Storage Bucket needs to be set up as the backend for the load balancer, which will allow traffic to be routed to the appropriate location.

## Backend

Our Backend will be hosted on Google Cloud Functions, so we need to enable `logging.googleapis.com` to allow our Cloud Functions’ telemetry to be accessible from the Google Cloud console. 

As we discussed in the previous section, Google Cloud Functions requires our source code to be uploaded, not compiled artifacts, this is because of the way Google Cloud Functions handles the packaging of our application on our behalf. As a result, this creates a dependency on `cloudbuild.googleapis.com`, which Cloud Functions uses to create a packaged artifact based on the source code we upload.

For our Google Cloud Function to execute, we need two additional Google APIs, the Cloud Run API (i.e., `run.googleapis.com`) and the Cloud Functions API (i.e., `cloudfunctions.googleapis.com`). Google Cloud Functions is a layer built onto the Cloud Run API that provides an additional layer of abstraction and additional capabilities to create event-driven workflows, while the Cloud Run API provides foundational service to run stateless containers that are invocable via HTTP requests.

Google Cloud Functions have a rather simple deployment model. Like AWS Lambda, you must declare a resource for the function itself. The resource has two main configuration components: the Build and Service Configuration.

	resource "google_cloudfunctions2_function" "backend" {
	
	  project  = google_project.main.project_id
	
	  name = "func-${var.application_name}-${var.environment_name}-backend-${random_string.project_id.result}"
	  location = var.primary_region
	  description = "a new function"
	
	}

The Build Configuration controls the type of execution runtime (e.g., Python, Java, or .NET), the entry point in the application code, and the location in storage where the application code can be found.

	  build_config {
	    runtime     = "dotnet6"
	    entry_point = "FleetAPI.Function"
	
	    source {
	      storage_source {
	        bucket = google_storage_bucket.backend.name
	        object = google_storage_bucket_object.deployment.name
	      }
	    }
	  }

The Service Configuration controls how many resources the Cloud Function has access to when invoked. Consequently, this configuration is also the primary driver of costs.

	  service_config {
	    max_instance_count = 1
	    available_memory   = "256M"
	    timeout_seconds    = 60
	  }

The Service Configuration block also allows you to set environment variables that can be used to pass non-sensitive configuration settings to the Cloud Function.

	  service_config {
	
	    ...
	
	    environment_variables = {
	        SERVICE_CONFIG_TEST = "config_test"
	    }
	  }

## Secrets Management

As we saw in previous chapters, we can only provision secrets using Google Cloud Secrets Manager once we have enabled the `secretmanager.googleapis.com` API.

First, we need to define the secret with a unique secret identifier that we can use to look up the secret’s value from our application code. If we are building multi-region deployments, we can also set the regions to which we want this secret replicated.

	resource "google_secret_manager_secret" "sauce" {
	  secret_id = "sauce"
	
	  replication {
	    user_managed {
	      replicas {
	        location = var.primary_region
	      }
	    }
	  }  
	}

As we saw with the `aws` provider in earlier chapters, the Secret is just a placeholder, a unique way to look up our secret’s value. We need to create versions of our secret to store the actual secret value.

	resource "google_secret_manager_secret_version" "sauce" {
	  secret = google_secret_manager_secret.secret.name
	
	  secret_data = "secret"
	  enabled = true
	}

After provisioning the secret and a version of our secret, we can access it from our Google Cloud Function. There are two methods for injecting our secrets into our Cloud Function. The first is using environment variables.

	    secret_environment_variables {
	      key        = "sauce"
	      project_id = google_project.main.project_id
	      secret     = google_secret_manager_secret.sauce.secret_id
	      version    = "latest"
	    }

The above code demonstrates how we can add a secret to the Service Configuration block of our Cloud Function to inject our secrets stored in the Google Secret Manager device using the secret’s identifier.

The second approach is probably more secure as it avoids exposing the secret within the process’s environment.

	    secret_volumes {
	      mount_path = "/etc/secrets"
	      project_id = google_project.main.project_id
	      secret     = google_secret_manager_secret.secret.secret_id
	    }

The above code shows how to set a mount point within the filesystem and drop the secret’s value there using the secret’s identifier.


[image-1]:	../images/CloudFunction-ResourceStructure.png
[image-2]:	../images/GCP-Function-Frontend-LoadBalancer.png