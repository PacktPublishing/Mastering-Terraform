# 3. Terraform Stacks

Terraform Stacks, a highly anticipated feature on the horizon for Terraform, promises to revolutionize the way we design and manage complex architectures across multiple control planes. This innovative feature is expected to provide a seamless and integrated experience for users working with Terraform Cloud and the Terraform Community Edition (the command line tool) by allowing for more sophisticated organization and modularization of infrastructure-as-code, Terraform Stacks aims to streamline the process of deploying and managing large-scale, multi-tiered environments. We’ll dig into what we know right now based on what was made public by HashiCorp at the time of writing and how it’s supposed to work when it’s eventually released.

## Current State

In the current landscape of Terraform usage, a singular root module serves as the cornerstone of infrastructure deployment. This root module encompasses the provider configuration and engages with various Terraform resources, either directly or via module references. The versatility of these root modules is heightened by supplying them with distinct input parameters tailored to the desired environment for deployment. To further segregate the deployment of each root module instance, Terraform workspaces are employed, resulting in individual Terraform state files. These state files are then uniquely associated with specific environments, such as DEV, TEST, or PROD, effectively encapsulating the configuration and status of the deployed infrastructure within each environment.

![Current State][image-1]
_Current State: Terraform Workspaces and Root Modules_

In the realm of provisioning complex environments with Terraform, it is often necessary to employ multiple root modules to delineate layers of architecture based on their dependencies, such as blast radius considerations or concrete control plane dependencies like those between a cloud platform and a Kubernetes control plane. This is not the only scenario where you will run into control plan dependencies within the providers, but it is a common one as the use of managed Kubernetes offerings continues to grow in popularity. The dependency can arise anytime you are provisioning resources with two or more providers and one provider provisions a resource that is then used to configure another Terraform provider. Depending on the way this dependent provider initializes, you might see a conflict arise because providers that rely on their control plane to be provisioned in another provider’s resource can experience deadlocks in both `terraform apply` and `terraform destroy`. This is because Terraform cannot plan resources on the control plane, which doesn’t exist yet. 

Some other common scenarios that I have encountered are with the Azure Managed Grafana Service which provisions a hosted Grafana instance on Azure. Although this is an Azure service, it is fully compatible with the Grafana endpoint, which means you can use the `grafana` provider for Terraform to provision resources for it. This mirrors the dependency created by Azure Kubernetes Service (also an Azure resource) and the `kubernetes` provider. It doesn’t matter what cloud platform you are working with. Many cloud platforms have similar managed service offerings that are provisioned through their corresponding provider, which produces an endpoint that can be automated by a Terraform provider designed for that control plane. This is even the case with something as fun as the `minecraft` provider—whether you are using EC2, Azure VMs, or GCE!

While there are two main approaches to this, both necessitate executing 'terraform apply' multiple times. The first approach involves provisioning each stage of our deployment independently and then linking the upstream dependency to the downstream stage using data sources, with values provided by input variables. This method allows different stages to be deployed relatively independently by various teams, but it introduces additional configuration management overhead, as each downstream dependency must explicitly reference the previously provisioned upstream stages. Consequently, this approach results in a highly serial deployment pattern, requiring each upstream dependency to be deployed and stabilized before progressing to the next downstream dependency.

![Current State][image-2]
_Current State: Independent Deployments with Data Source Dependencies_

An alternative approach to provisioning complex environments with Terraform diverges from independent deployments and instead adopts a monolithic pipeline that sequentially executes `terraform apply`. In this model, the dependencies are seamlessly integrated by piping the Terraform outputs from the upstream dependency directly into the inputs of the downstream dependency. While this method streamlines automation, it also results in a tighter coupling of the environments. Irrespective of the approach—whether it involves independent deployments or a monolithic pipeline—there is an inherent necessity to implement a substantial amount of "glue" to stitch together multiple 'terraform apply' steps. This entails writing bash scripts or similar automation to act as the connective tissue, ensuring the correct values are passed from one pipeline job to the next, thereby maintaining the integrity of the deployment process across various stages.

![Current State][image-3]
_Current State: Integrated Deployment with Output-based Dependencies_

## Stacks

Defined in a `*.tfstack` file, Stacks allow you to declare one or more `component` blocks that essentially define what is currently a Root Module. These components represent discrete and deterministic provisioning stages within a deployment. 

![Terraform Stacks][image-4]
_Future State: Terraform Stacks_

In the diagram above, we see three components that make up our stack:

1. Network Infrastructure
2. Compute Infrastructure
3. Kubernetes Deployments

This would be defined in a `*.tfstack` file in this manner:

	component "network" {
	  source = "./network"
	
	  inputs = {
	    region = var.region
	  }
	
	  providers = {
	    aws = providers.aws.this
	  }
	}

The compute infrastructure would be defined in the same file but this time referencing outputs from the network component that it draws a dependency on. This informs Terraform to provision the network component first and resolve that stage of the deployment first before attempting to deploy the compute infrastructure component next.

	component "compute" {
	  source = "./compute"
	
	  inputs = {
	    region = var.region
	    network_name = component.network.network_name
	  }
	
	  providers = {
	    aws = providers.aws.this
	  }
	}

After the compute component is provisioned we will have a Kubernetes cluster that is ready to deploy our applications and services to. Therefore, we declare the final component of our stack, the application component.

	component "app" {
	  source = "./app"
	
	  inputs = {
	    region = var.region
	    cluster_name = component.compute.cluster_name
	  }
	
	  providers = {
	    aws = providers.aws.this
	    kubernetes = providers.kubernetes.this
	    helm = providers.helm.this
	  }
	}

This allows us to initialize the `kubernetes` and `helm` providers only after the necessary steps have been taken to provision the Kubernetes cluster, which is absolutely required before we can even begin to execute a plan.

## Deployments

Defined in a `*.tfdeploy` file, Deployments allow you to declare one or more `deployment` blocks that essentially define what is currently a Terraform Workspace that manifests itself once provisioned into an individual Terraform State file that represents a provisioned environment. The introduction of Deployments allows us to declaratively establish different environments that we provision in our configuration rather than implicitly through the organization of our Terraform Workspaces and the context in which we execute Terraform core workflow operations like `plan` and `apply`.

Deployments act as the central place where provider configuration is established. This includes linking the preferred method of authentication with each provider. This is done using a new block called `identity_token`, which would be defined in this manner for AWS:

	identity_token "aws" {
	  audience = ["aws.workload.identity"]
	}

This would be defined in a `*.tfdeploy` file in this manner:

	deployment "dev" {
	  variables = {
	    region = "us-west-2"
	    identity_token_file = identity_token.aws.jwt_filename
	  }
	}

As you can see, the deployment block allows us to establish multiple instances of our Stacks and configure them with their own input variables and provider context, including relevant authentication and authorization context.

Terraform Stacks is an exciting new capability in preview on Terraform Cloud and is planned to be released for both Terraform Cloud and Terraform Community Edition (the Terraform command line tool). As you can see, with this approach, we’ll be able to eliminate a tremendous amount of “plumbing” that we currently put into our pipelines (i.e., GitHub Actions, Azure DevOps, Jenkins, etc.) and replace it with Terraform configuration that we can manage with the GitFlow standards we learned about in Chapter 6. If you plan on managing complex solutions with Terraform, this is a feature to watch for in future releases!

[image-1]:	../images/TerraformStacks-CurrentState.png
[image-2]:	../images/TerraformStacks-CurrentState-Stages.png
[image-3]:	../images/TerraformStacks-CurrentState-Pipeline.png
[image-4]:	../images/TerraformStacks-FutureState.png