# 3. Terraform

As we discussed in our design, our solution is made up of two Application Components: the front end and the back end. Each has its own application codebase that needs to be deployed. Since this is the first time we will be using the `aws` provider, we’ll look at the basic provider setup and the configuration of the backend before we move into the nuts and bolts of each component of our architecture.

## Provider Setup

We need to specify all the providers that we intend to use in this solution within the `required_providers` block.

	terraform {
	  required_providers {
	    aws = {
	      source  = "hashicorp/aws"
	      version = "~> 5.17"
	    }
	    cloudinit = {
	      source  = "hashicorp/cloudinit"
	      version = "~> 2.3.2"
	    }
	  }
	}

We’ll also configure the AWS provider to ensure that it uses the desired target region using an input variable `primary_region`.

	provider "aws" {
	  region = var.primary_region
	}

Sometimes, you may want to add a secondary region in the future, so it’s a good idea to establish the primary region when you start the project. Even if you only deploy to one region, you still have a “Primary Region”. 

The AWS provider does require some additional parameters to specify the credentials to use to connect to AWS, but because these are sensitive values, we don’t want to embed them into the code. We’ll pass those values in later when we automate the deployment using the standard AWS environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`. It’s important to note that there are many different ways to configure the AWS provider to authenticate with AWS. I recommend using environment variables as it is a consistent approach across cloud platforms and other Terraform providers, and it integrates easily with different pipeline tools, like GitHub Actions, which we’ll be using in the next section and future chapters.

## Backend

Because we will be using a CI/CD pipeline to provision and maintain our environment long term we need to setup a remote backend for our Terraform State. Because our solution will be hosted on AWS, we’ll use the AWS Simple Storage Service (S3) backend to store our Terraform State.

Just like the AWS provider, we don’t want to hard code the backend configuration in our code so we simply setup a placeholder for the backend.

	terraform {
	
	  ...
	
	  backend "s3" {
	  }
	}

We’ll configure the backend’s parameters using the `-backend-config` parameters when we run `terraform init` in our CI/CD pipeline.

It’s important to ensure that the AWS IAM identity you use to authenticate with AWS has access to this S3 bucket. Otherwise, you will get authentication errors.

## Input Variables

It’s good practice to pass in short names that identify the application’s name and the application’s environment. This allows you to embed consistent naming conventions across the resources that make up your solution, which makes it easier to identify and track resources from the AWS Console.

The `primary_region`, `vpc_cidr_block` and `az_count` input variables drive key architectural characteristics of the deployment. It is important that they are not hard-coded, as it would limit the reusability of the Terraform codebase. 

The `vpc_cidr_block` establishes the Virtual Network address space, which is often tightly regulated by an enterprise governance body. There is usually a process to ensure that teams across an organization do not use IP address ranges that conflict, thus making it impossible in the future to allow those two applications to integrate with one another or integrate with shared network resources within the enterprise.

The `az_count` allows for configurability of how much redundancy we want within our solution. This will affect the high availability of the solution but also the cost of the deployment. As you can imagine, cost, is also a tightly regulated characteristic of cloud infrastructure deployments.

## Consistent Naming & Tagging

The AWS Console is designed in such a way that it’s rather difficult to get an application-centric view of your deployment. This is why it’s extremely important to leave breadcrumbs within the resources that you deploy that indicate what application and what environment they belong to. Almost all resources within the AWS provider have a `map` attribute called `tags`. 

	resource "aws_vpc" "main" {
	  cidr_block = var.vpc_cidr_block
	
	  tags = {
	    Name        = "${var.application_name}-${var.environment_name}-network"
	    application = var.application_name
	    environment = var.environment_name
	  }
	}

You should make a habit of setting both the AWS Console recognized `Name` tag and a tagging scheme of your own devices that establishes Application and Environment ownership of that resource. For our solution, we use two top-level input variables, `application_name` and `environment_name`, to set this context, and we embed these values on all the resources that we provision.

AWS does have the ability to create an application-centric view within the AWS Console using something called a Resource Group. Unlike on other platforms, a resource group on AWS is not a strong boundary around a set of resources but a loosely coupled relationship between resources derived from a common tagging scheme.

	resource "aws_resourcegroups_group" "main" {
	  name = "${var.application_name}-${var.environment_name}"
	
	  resource_query {
	    query = jsonencode(
	      {
	        ResourceTypeFilters = [
	          "AWS::AllSupported"
	        ]
	        TagFilters = [
	          {
	            Key    = "application"
	            Values = [var.application_name]
	          },
	          {
	            Key    = "environment"
	            Values = [var.environment_name]
	          }
	        ]
	      }
	    )
	  }
	}

The above code creates an AWS resource group that creates a central location where you can access all of your related resources from one place. Simply adding `application` and `environment` tags to all your resources will include them.

## Virtual Network

Because our solution is a standard three-tier architecture, we are configuring our virtual network into public and private subnets for the front and back end application components. 

We want to distribute our Virtual Machines across the Availability Zones in order to ensure the high availability of our solution. Rather than hard-code the Availability Zones or just take the first two, we can randomly select the number of Availability Zones we want from the list of available ones for the given region using the `aws_availability_zones` Data Source and a `random_shuffle` resource from the `random` provider.

	data "aws_availability_zones" "available" {
	  state = "available"
	}
	
	resource "random_shuffle" "az" {
	  input        = data.aws_availability_zones.available.names
	  result_count = var.az_count
	}

If the `az_count` input variable has a value of `2`, then the above code will randomly select two Availability Zones from the region of the current AWS provider. Remember that the AWS provider is scoped to a particular Region, and when we initialized the provider, we used the `primary_region` input variable to set that value.

Rather than hard-code the address space for our subnets, it would be nice if we could calculate our subnets’ address space using HashiCorp Configuration Language’s built-in functions. The `cidrsubnet` function allows us to take an address space and split it into smaller address spaces.

	locals {
	  azs_random = random_shuffle.az.result
	
	  public_subnets = { for k, v in local.azs_random :
	    k => {
	      cidr_block        = cidrsubnet(var.vpc_cidr_block, var.cidr_split_bits, k)
	      availability_zone = v
	    }
	  }
	  private_subnets = { for k, v in local.azs_random :
	    k => {
	      cidr_block        = cidrsubnet(var.vpc_cidr_block, var.cidr_split_bits, k + var.az_count)
	      availability_zone = v
	    }
	  }
	}

The above code will generate two maps, one for the public subnets and another for the private subnets. It accomplishes this by taking the randomly selected Availability Zones and for each using the `cidrsubnet` to grab the next available block of `/24` or `256` IP addresses (more than enough for our application to scale to a huge number of Virtual Machines in each Availability Zone in both the Frontend and the Backend.

	public_subnets = {
	  "0" = {
	    "availability_zone" = "us-west-2c"
	    "cidr_block" = "10.0.0.0/24"
	  }
	  "1" = {
	    "availability_zone" = "us-west-2a"
	    "cidr_block" = "10.0.1.0/24"
	  }
	}
	private_subnets = {
	  "0" = {
	    "availability_zone" = "us-west-2c"
	    "cidr_block" = "10.0.2.0/24"
	  }
	  "1" = {
	    "availability_zone" = "us-west-2a"
	    "cidr_block" = "10.0.3.0/24"
	  }
	}


The above is the value that the `public_subnets` and `private_subnets` maps will have when evaluated with a `vpc_cidr_block` of `10.0.0.0/16`, `cidr_split_bits` of `8` and an `az_count` of `2`.

By manipulating these input variables, we can reasonably size the Virtual Network and its corresponding subnets so that we don’t monopolize available address spaces for other applications that we may want to provision within the broader organization. For example, setting the `vpc_cidr_block` to `10.0.0.0/22` allocates total IP addresses of `1024` to our application. With an `az_count` of `2` and a `cidr_split_bits` value of `2`, we can allocate address space for our four subnets, each with a `/24` and `256` IP addresses. This gives us sufficient room for our application to scale without overallocating valuable IP Address space.

	
	resource "aws_subnet" "frontend" {
	
	  for_each = local.public_subnets
	
	  vpc_id            = aws_vpc.main.id
	  availability_zone = each.value.availability_zone
	  cidr_block        = each.value.cidr_block
	
	}

We create each subnet by iterating over the corresponding map of subnet address spaces. The above code demonstrates how we can use this map to set the correct availability zone and address space for each subnet.

## Network Routing

As per our design, the public subnets route internet traffic to the Internet Gateway.

	resource "aws_route_table" "frontend" {
	  vpc_id = aws_vpc.main.id
	
	  route {
	    cidr_block = "0.0.0.0/0"
	    gateway_id = aws_internet_gateway.main.id
	  }
	}
	
	resource "aws_route_table_association" "frontend" {
	
	  for_each = aws_subnet.frontend
	
	  subnet_id      = each.value.id
	  route_table_id = aws_route_table.frontend.id
	
	}

We use the `aws_route_table` resource to define the route and then the `aws_route_table_association` to link the Route Table to the corresponding subnet.

The private subnets route their internet traffic to a NAT Gateway, which is provisioned in each private subnet.

	resource "aws_eip" "nat" {
	
	  for_each = local.private_subnets
	
	}
	
	resource "aws_nat_gateway" "nat" {
	
	  for_each = local.private_subnets
	
	  allocation_id = aws_eip.nat[each.key].id
	  subnet_id     = aws_subnet.backend[each.key].id
	
	  depends_on = [aws_internet_gateway.main]
	
	}

Because each private subnet has its own NAT Gateway, we need a route table for each subnet to route the traffic to the correct NAT Gateway.

	resource "aws_route_table" "backend" {
	
	  for_each = local.private_subnets
	
	  vpc_id = aws_vpc.main.id
	
	  route {
	    cidr_block     = "0.0.0.0/0"
	    nat_gateway_id = aws_nat_gateway.nat[each.key].id
	  }
	}
	
	resource "aws_route_table_association" "backend" {
	
	  for_each = local.private_subnets
	
	  subnet_id      = aws_subnet.backend[each.key].id
	  route_table_id = aws_route_table.backend[each.key].id
	
	}

Notice that, unlike the public subnets, which share the same Route Table, we need to iterate on the `private_subnets` map to create a different Route Table for each private subnet and associate it to the corresponding private subnet using the `each` symbol.

## Load Balancing

As per our design, we need two AWS Application Load Balancers, one for the front end and another for the back end. We’ll use the `aws_lb` resource and related resources with the `aws_lb` prefix to provision the Target Group and Listener configuration.

	resource "aws_lb_target_group" "frontend_http" {
	
	  name                          = "${var.application_name}-${var.environment_name}-frontend-http"
	  port                          = 5000
	  protocol                      = "HTTP"
	  vpc_id                        = aws_vpc.main.id
	  slow_start                    = 0
	  load_balancing_algorithm_type = "round_robin"
	
	  stickiness {
	    enabled = true
	    type    = "lb_cookie"
	  }
	
	  health_check {
	    enabled             = true
	    port                = 5000
	    interval            = 30
	    protocol            = "HTTP"
	    path                = "/"
	    matcher             = 200
	    healthy_threshold   = 3
	    unhealthy_threshold = 3
	  }
	
	}

Notice that the Sticky Session configuration needed for the ASP.NET Core Blazor Web application’s WebSocket configuration is implemented by a nested `stickiness` block. Likewise, the health probe is implemented by a nested `health_check` block. This structure will be identical for both the front end and the back end, but the configuration will differ slightly, with the back end not requiring sticky sessions and having a different path for the health probe.

The Virtual Machines are explicitly included in the Target Group using the `aws_lb_target_group_attachment` resource.

	resource "aws_lb_target_group_attachment" "frontend_http" {
	
	  for_each = aws_instance.frontend
	
	  target_group_arn = aws_lb_target_group.frontend_http.arn
	  target_id        = each.value.id
	  port             = 5000
	
	}

Notice that we are iterating over the corresponding `aws_instance` resource map and referencing the AWS EC2 Instance ID using `each.value.id`. 

 Finally, we provision the AWS Application Load Balancer itself.

	resource "aws_lb" "frontend" {
	  name               = "${var.application_name}-${var.environment_name}-frontend"
	  internal           = false
	  load_balancer_type = "application"
	  subnets            = [for subnet in values(aws_subnet.frontend) : subnet.id]
	  security_groups    = [aws_security_group.frontend_lb.id]
	
	  tags = {
	    Name        = "${var.application_name}-${var.environment_name}-frontend-lb"
	    application = var.application_name
	    environment = var.environment_name
	  }
	
	}

Notice that we are dynamically constructing a `list` of subnets using the corresponding `aws_subnet` resource map. When a resource block is provisioned with a `count` that resource block becomes a `list`, when it is provisioned with a `for_each` iterator it becomes a `map`. This is important detail to pay attention to when you want to reference it from other resources.

Lastly, we connect our AWS Application Load Balancer to the Target Group using the Listener.

	resource "aws_lb_listener" "frontend_http" {
	
	  load_balancer_arn = aws_lb.frontend.arn
	  port              = "80"
	  protocol          = "HTTP"
	
	  default_action {
	    type             = "forward"
	    target_group_arn = aws_lb_target_group.frontend_http.arn
	  }
	}

## Network Security

As per our design, we have five logical components of our solution architecture through which network traffic will pass. Each needs its own Security Group and set of Rules to allow ingress and egress traffic.

	resource "aws_security_group" "frontend_lb" {
	  name        = "${var.application_name}-${var.environment_name}-frontend-lb-sg"
	  description = "Security group for the load balancer"
	  vpc_id      = aws_vpc.main.id
	}

A Security Group is created using the `aws_security_group` resource and attached to a Virtual Network. 

Not all components within the architecture will need both ingress and egress rules, but it’s important to think through all the ways network traffic should be allowed to flow through the system.

	resource "aws_security_group_rule" "frontend_lb_ingress_http" {
	  type              = "ingress"
	  from_port         = 80
	  to_port           = 80
	  protocol          = "tcp"
	  security_group_id = aws_security_group.frontend_lb.id
	  cidr_blocks       = ["0.0.0.0/0"]
	}
	resource "aws_security_group_rule" "frontend_lb_egress_http" {
	  type                     = "egress"
	  from_port                = 5000
	  to_port                  = 5000
	  protocol                 = "tcp"
	  security_group_id        = aws_security_group.frontend_lb.id
	  source_security_group_id = aws_security_group.frontend.id
	}

The above code establishes the rules we designed for the Frontend Load Balancer which allows traffic in from the internet (e.g., `0.0.0.0/0`) and allows traffic out to the Frontend Virtual Machines (e.g., `aws_security_group.frontend.id`). 

## Secrets Management

In order to allow our Virtual Machines access to our AWS Secrets Manager resources, we need to define an IAM Role and associate it with our Virtual Machines. This allows your Virtual Machines to operate under the security context defined by the IAM Policies attached to this IAM Role.

	resource "aws_iam_role" "backend" {
	  name = "${var.application_name}-${var.environment_name}-backend"
	
	  assume_role_policy = jsonencode({
	    Version = "2012-10-17"
	    Statement = [
	      {
	        Action = "sts:AssumeRole"
	        Effect = "Allow"
	        Sid    = ""
	        Principal = {
	          Service = "ec2.amazonaws.com"
	        }
	      },
	    ]
	  })
	}

The above code creates the IAM Role for the Backend Virtual Machines, which need access to the PostgreSQL Database’s connection string that we will store in AWS Secrets Manager. The IAM Role itself doesn’t do anything unless there is a Policy defined. We need to attach a Policy Definition to the Role to grant specific privileges to the Virtual Machines.

	resource "aws_iam_role_policy" "backend" {
	  name = "${var.application_name}-${var.environment_name}-backend"
	  role = aws_iam_role.backend.id
	
	  policy = jsonencode({
	    Version = "2012-10-17"
	    Statement = [
	      {
	        Action = [
	          "secretsmanager:GetSecretValue",
	        ]
	        Effect   = "Allow"
	        Resource = "arn:aws:secretsmanager:secret:${var.application_name}/${var.environment_name}/*"
	      },
	    ]
	  })
	}

The above code grants access to all Virtual Machines operating with this IAM Role associated with accessing AWS Secrets Manager Secrets that begin with the `fleet-ops/dev` prefix. We build this prefix using our standard naming convention input variables `application_name` and `environment_name`, which have the values of `fleet-ops` and `dev,` respectively. When we provision the production version of the `fleet-ops` platform, the `environment_name` input variable will be set to `prod`—thus ensuring that the Virtual Machines in the `dev` environment don’t have access to the secrets in the `prod` environment. Deploying the different environments of our application into isolated AWS accounts would also create a more secure security boundary.

## Virtual Machines

When provisioning static Virtual Machines, we have much more control over the configuration of each machine. Some Virtual Machines have specific network and storage configurations to meet workload demands.

	resource "aws_network_interface" "frontend" {
	
	  for_each = aws_subnet.frontend
	
	  subnet_id = each.value.id
	}
	
	resource "aws_network_interface_sg_attachment" "frontend" {
	
	  for_each = aws_instance.frontend
	
	  security_group_id    = aws_security_group.frontend.id
	  network_interface_id = each.value.primary_network_interface_id
	
	}

The above code creates a Network Interface that we can then attach to a Virtual Machine. Notice that we are iterating over the Frontend subnet’s, this will ensure we have exactly one Virtual Machine in each subnet (and consequently each Availability Zone). This Network Interface is where we attach the Security Group for Virtual Machines in the Frontend. 

Finally, we provision the Virtual Machine using the `aws_instance` resource, taking care to use the correct instance type, Network Interface, and AWS AMI.

	resource "aws_instance" "frontend" {
	
	  for_each = aws_subnet.frontend
	
	  ami           = data.aws_ami.frontend.id
	  instance_type = var.frontend_instance_type
	  key_name      = data.aws_key_pair.main.key_name
	  user_data     = data.cloudinit_config.frontend.rendered
	  monitoring    = true
	
	  network_interface {
	    network_interface_id = aws_network_interface.frontend[each.key].id
	    device_index         = 0
	  }
	
	}

AWS has a cross-cutting service called CloudWatch that collects logs and telemetry across the various AWS services. To enable CloudWatch on your EC2 instances, you simply need to add the `monitoring` attribute and set it to `true`. 

## Monitoring

Depending on the service and its available configuration options within the Terraform resources used to provision it, in order to activate CloudWatch you might need to go through a process of provisioning additional resources and setting up additional IAM permissions to grant the respective resource to write to CloudWatch.

The first thing we need to setup is an IAM Policy that will grant the specific service access to assume an IAM Role. In this case, we are granting access to VPC Flow Logs access to assume an IAM Role.

	
	data "aws_iam_policy_document" "vpc_assume_role" {
	  statement {
	    effect = "Allow"
	
	    principals {
	      type        = "Service"
	      identifiers = ["vpc-flow-logs.amazonaws.com"]
	    }
	
	    actions = ["sts:AssumeRole"]
	  }
	}

We’ll use this policy when setting up the IAM Role to grant the VPC Flow Logs service access to this particular IAM Role. This will be important later when we link everything together.

	resource "aws_iam_role" "vpc" {
	  name               = "${var.application_name}-${var.environment_name}-network"
	  assume_role_policy = data.aws_iam_policy_document.assume_role.json
	}

The above code allows VPC Flow Logs to assume this role, eventually granting it access to writing logs to CloudWatch.

Next, we need to set up another IAM Policy that will grant access to write to CloudWatch logs. You can further narrow the scope of an access policy by narrowing the allowed actions and the allowed resources the policy grants access to. 

	
	data "aws_iam_policy_document" "cloudwatch" {
	  statement {
	    effect = "Allow"
	
	    actions = [
	      "logs:CreateLogGroup",
	      "logs:CreateLogStream",
	      "logs:PutLogEvents",
	      "logs:DescribeLogGroups",
	      "logs:DescribeLogStreams",
	    ]
	
	    resources = ["*"]
	  }
	}

In the above code, we do a good job of being specific about the types of operations we want to grant access to by giving specific operations such as `logs:PutLogEvents`. However, the resources are set to `*`, a very wide access level. We should consider narrowing that down to just the resources that we need.

The next step is to attach the policy to the IAM Role. 

	resource "aws_iam_role_policy" "cloudwatch" {
	  name   = "${var.application_name}-${var.environment_name}-network-cloudwatch"
	  role   = aws_iam_role.vpc.id
	  policy = data.aws_iam_policy_document.cloudwatch.json
	}

At this point, we have an IAM Role that is allowed to write to CloudWatch and we have allowed VPC Flow Logs to assume this role.

Next, we need to create a CloudWatch Log Group that will store the logs from the VPC.

	resource "aws_cloudwatch_log_group" "vpc" {
	  name = "${var.application_name}-${var.environment_name}-network"
	}

Finally, we’ll connect the VPC Flow Logs to the Log Group and assign the IAM Role it should use to gain access to write the CloudWatch.

	resource "aws_flow_log" "main" {
	  iam_role_arn    = aws_iam_role.vpc.arn
	  log_destination = aws_cloudwatch_log_group.vpc.arn
	  traffic_type    = "ALL"
	  vpc_id          = aws_vpc.main.id
	}

The above code also links our VPC to the VPC Flow Log Service, thus completing the flow of the networking logs into the corresponding CloudWatch Log Group.