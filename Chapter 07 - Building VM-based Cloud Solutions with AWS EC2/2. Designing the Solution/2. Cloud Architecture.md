# 2. Cloud Architecture

The first part of our design is adapting our solution’s architecture to the target cloud platform: AWS. This involves mapping application architecture components to AWS services and thinking through the configuration of those services to meet the requirements of our solution.

## Virtual Network

Virtual Machines must be deployed within a Virtual Network. On AWS, we use the AWS EC2 service to provide our virtual machines, and we use the AWS Virtual Private Cloud (VPC) to provide our virtual network. When working on AWS, the term “EC2 instance” is used interchangeably with the term “Virtual Machine.” Likewise, the term “VPC” is used interchangeably with the term “Virtual Network”. In this book, I will try to use industry-standard terminology wherever possible. You should get in the habit of thinking this way as this will allow your knowledge and skills to better transition between the different cloud platforms.

![Resource][image-1]
_AWS Virtual Network Architecture_

As we’ve discussed previously, a Virtual Network is divided up into a set of subnets. On AWS, a Virtual Network is scoped to a specific Region, and a subnet is scoped to an Availability Zone within that region. Therefore, in order to build highly available systems on AWS, it’s important that we distribute our workloads across multiple Availability Zones. Therefore, if one Availability Zone experiences an outage, our workload deployed into the other Availability Zone will prevent disruption to the end users.

Our application’s Virtual Machines need to be provisioned into subnets within a Virtual Network. The front end of our application needs to be accessible over the internet, while the back end only needs to be accessible to the front end. Therefore, we should provision separate subnets for the internet-accessible front end and our private back end. This is a common pattern of creating “public” and “private” subnets.

![Resource][image-2]
_Public & Private Subnets for the Frontend and Backend Application Components_

In this pattern, two pairs of public and private subnets are created. Each pair is provisioned in the same Availability Zone. The reason why each pair shares the same Availability Zone is due to the dependency between the Frontend and the Backend. For example, if there is an outage affecting the Availability Zone of the Backend, the Frontend won’t be able to operate anyway. Likewise, if there is an outage affecting the Availability Zone of the Frontend, no traffic will be routed to the Backend anyway. We can create as many pairs of these public/private subnets as there are Availability Zones within a Region. Most regions have four to five Availability Zones, but usually, two to three Availability Zones are sufficient for most workloads. After that, you are more likely to benefit from setting up a multi-region deployment.

## Network Routing

There are a few other components that we need to set up within this Virtual Network to enable our Virtual Machines to function properly. In AWS, when you provision a Virtual Machine into a Virtual Network, you won’t have internet access! For most connected applications, this is required to allow connectivity to third-party services or, at the very least, an inconvenience for operators as they will be unable to perform Operating System upgrades and patches using internet-hosted package repositories.

![Resource][image-3]
_Internet and NAT Gateways enable Internet Access for Virtual Machines within the subnets_

The Internet Gateway is attached to the Virtual Network at the Region level, providing Internet access to the entire VPC, while the NAT Gateways are deployed into each public subnet at the Availability Zone level to enable EC2 instances in private subnets access to the Internet without being directly accessible from the Internet. Each NAT Gateway also needs its own static public IP Address to grant access. This is achieved using the Elastic IP service on AWS.

![Resource][image-4]
_Route Tables associated with the subnets direct traffic to the correct Gateway_

The last step to establishing internet access to our Virtual Machines in private subnets is routing internet-bound traffic to the correct NAT gateway for each subnet, while Virtual MAchines in public subnets can directly access the Internet. This is done using Route Tables. In the public subnet, we route internet traffic to the Internet Gateway. In the private subnet, we route internet traffic to the NAT Gateway.

## Load Balancing

Now that our subnets are setup and connected using proper Routing Tables we can provision our Virtual Machines. In order to achieve high availability we need at least one Virtual Machine provisioned to each subnet for both the Frontend and the Backend of our solution. We can increase the number of Virtual Machines in each subnet in order to achieve even more reliability or scale.

![Resource][image-5]
_Virtual Machines provisioned to our Virtual Network_

The problem with the current design is that we need a way for our system to respond correctly to an outage affecting one of our Availability Zones. This is where a Load Balancer comes in. It allows us to get the double benefit of routing traffic to healthy endpoints and distributing the load evenly across our resources.

In AWS, the Application Load Balancer (ALB) service performs this function. The Load Balancer’s job is to be the single point of contact for clients to send requests to. The load balancer then forwards that traffic to Virtual Machines and routes the corresponding responses back to the client from where the request originated.

![Resource][image-6]
_Load Balancer forwarding traffic to Virtual Machines across Availability Zones_

In the AWS Application Load Balancer, the first thing we need to set up is the Listener. On the Listener, you specify a port, a protocol, and one or more actions you want to take when a request is received. The most basic type of action is to forward the request to a Target Group. 

In our solution, the Target Group will consist of a set of Virtual Machines. The Target Group specifies what port and protocol the request should be sent to, as well as a health probe with a specific application path. The health probe can optionally be set up on a different port and protocol, and it provides a number of different settings to control how frequently to be probed and how to evaluate whether the endpoint is healthy or unhealthy. Healthy is usually indicated by an HTTP Status Code of 200. Anything else is considered unhealthy.

For both our Frontend and Backend, we have a simple set of Virtual Machines for the Target Group with an endpoint configured for HTTP protocol on port 5000 (the default port for ASP.NET Core). 

The Frontend is an ASP.NET Core Blazor Application. As a result, it uses SignalR (which abstracts WebSocket communication) to perform real-time connectivity between the web browser and the server. As a result, we need to enable sticky sessions so that this can function properly. Sticky sessions will allow the client to continue to use the same Virtual Machine, thus allowing the WebSocket to stay alive and not be disrupted by changing which web server it communicates with. 

For the health probe, the Frontend will use the root path of the web application `/` and the Backend will use a special path that routes to a Controller configured to respond to the health probe.

## Network Security

Now that our Virtual Network is fully configured and our Virtual Machines are setup behind Load Balancers we need to think through what network traffic we want to allow through the system. In AWS, this is controlled by creating Security Groups that allow traffic between to components of your architecture on specific ports using specific protocols. 

The first step in this process is to think through the logical stops for our network traffic as it makes its way through our solution.

![Resource][image-7]
_Logical Components of our Architecture_

The Application Components, including the front end and the back end, are clearly on this list, followed by the Database. However, these aren’t the only places where our network traffic flows through. Since we introduced load balancers in front of both the front end and the back end, we have had two additional stops for network traffic.

The next step is to think through how each component communicates with others. This includes both the port and protocol but also the direction of the traffic. In order to do this you need to think about the network traffic from the perspective of each component.

![Resource][image-8]
_Frontend Load Balancer network traffic flow_

From the perspective of the Frontend Load Balancer, we’ll be receiving traffic from the internet on port 80 using the HTTP protocol. This inbound traffic is called ‘ingress’. Due to the Target Group configuration, we’ll be forwarding those requests to the front end on port 5000 using the HTTP protocol. This outbound traffic is called ‘egress’.

![Resource][image-9]
_Frontend network traffic flow_

From the perspective of the front end, we’ll be receiving traffic from the front end Load Balancer on port 5000 using the HTTP protocol. The C# application code will make requests to the REST Web API hosted in the backend, but we’ll be routing all our requests to the backend through the backend load balancer on port 80 using the HTTP protocol.

![Resource][image-10]
_Backend Load Balancer network traffic flow_

From the perspective of the Backend Load Balancer, we’ll be receiving traffic from the Frontend on port 80 using the HTTP protocol. Due to the Target Group configuration, we’ll be forwarding those requests to the Backend on port 5000 using the HTTP protocol.

![Resource][image-11]
_Backend network traffic flow_

From the Perspective of the Backend, we’ll be receiving traffic from the Backend Load Balancer on port 5000 using the HTTP protocol. The C# application code will be making requests to the PostgreSQL Database on port 5432 using the HTTPS protocol.

## Secrets Management

Secrets such as database credentials or service access keys need to be stored securely. Each cloud platform has its own service that provides this functionality. On AWS, this service is called AWS Secrets Manager. 

![Resource][image-12]
_Secrets stored in AWS Secrets Manager can be accessed by Virtual Machines once they have the necessary IAM privileges_

You simply create secrets on this service using a consistent naming convention, then construct an IAM role that has permission to access these secrets. The below IAM policy will grant permission to just secrets that start with `fleetportal/`. 

	{
	    "Version": "2012-10-17",
	    "Statement": [
	        {
	            "Effect": "Allow",
	            "Action": "secretsmanager:GetSecretValue",
	            "Resource": "arn:aws:secretsmanager:region:account-id:secret:fleetportal/*"
	        }
	    ]
	}

The values for `region` and `account-id` would need to be altered to reflect where the secrets were actually created within. It’s important to note that an AWS account is typically used as a security boundary for an application and an environment. So, we would likely have separate AWS accounts for our solution’s Development, Production, and any other environment we may need. This would isolate our Secret Manager Secrets within the context of the AWS Account and the Region.

The two main attributes we use to grant permissions are `action` and `resource`. When implementing the principle of least privilege, it’s important to be as specific as possible about exactly the actions required for a particular identity. If the access is not required—don’t grant it. Likewise, we should narrow the resources we grant these permissions to be as narrow as possible. It’s easy to be lazy and leave a `*` in the resources or the actions. Still, we need to be aware that a malicious attacker could use overly generous permissions to move laterally within our environments.

## Virtual Machines

Now that we have everything we need for our solution, we can finish by talking about where our application components will actually run: Virtual Machines provisioned using AWS Elastic Cloud Compute (EC2) service.

When provisioning Virtual Machines on AWS, you have two options. First, you can provide static virtual machines. In this approach, you need to specify key characteristics for every Virtual Machine. Alternatively, you can use an AWS Auto-Scaling Group to dynamically provision and manage the Virtual Machines. In this approach, you provide the Auto-Scaling Group with some configuration and parameters on when to scale up and when to scale down, and the Auto-Scaling Group will take care of everything else.

When provisioning a static Virtual Machine on AWS, you need to associate it with an AWS Key Pair to ensure that you can connect to its Operating System. This will allow your operators to perform diagnostics and update or patch the software and Operating System.

All Virtual Machines need to be connected to a Virtual Network, so when you set up a static Virtual Machine, you need to specify the network configuration. This is accomplished by creating a Network Interface and associating it with the Virtual Machine. The Network Interface connects the Virtual Machine to the appropriate subnet, which is the place where you attach one or more Security Groups.

The internal configuration of your Virtual Machine is controlled by two critical attributes: the Virtual Machine Image and the User Data. As we discussed in Chapter 4, the Virtual Machine Image can either be a vanilla installation of an Operating System or it can be a fully configured version of your application. The decision of “Build vs. Bake” is up to you.

User Data allows you to run the “last mile” configuration when the Virtual Machine starts up. This can be done using industry standard Cloud Init configuration to perform a wide variety of tasks such as user / group setup, setting up environment variables or mounting disks.

![Resource][image-13]
_Virtual Machines created statically_

AWS has the ability to dynamically manage your Virtual Machines based on the load that they incur. This is done using an Auto-Scaling Group. The Auto-Scaling Group is responsible for provisioning the Virtual Machines. Consequently, that means that the Auto-Scaling Group needs to have the key characteristics that define your Virtual Machine set on its Launch Template. The Auto-Scaling Group uses this Launch Template to specify the configuration of each Virtual Machine that it provisions.

![Resource][image-14]
_Virtual Machines created and managed dynamically using an Auto-Scaling Group_

Besides this Launch Template, the Auto-Scaling Group simply needs to be told what subnets the Virtual Machines should be provisioned into and under what circumstances it should provision or de-provision Virtual Machines from the set that it actively manages.

## Monitoring

AWS has a cross-cutting service called CloudWatch that can capture logs and telemetry from various AWS services you consume within your solutions. We’ll be using this as the primary logging mechanism within this book. Many services support CloudWatch out-of-the-box with minimal to no configuration to get it working. At the same time, other services and scenarios require permissions to be granted to allow that service to log into CloudWatch.


[image-1]:	../images/AWS-VirtualNetwork.png
[image-2]:	../images/AWS-VirtualNetwork-PublicPrivate.png
[image-3]:	../images/AWS-VirtualNetwork-InternetAccess.png
[image-4]:	../images/AWS-VirtualNetwork-RouteTables.png
[image-5]:	../images/AWS-VirtualMachines.png
[image-6]:	../images/AWS-LoadBalancer.png
[image-7]:	../images/AWS-SecurityGroups-Overview.png
[image-8]:	../images/AWS-SecurityGroup-Frontend-LoadBalancer.png
[image-9]:	../images/AWS-SecurityGroup-Frontend.png
[image-10]:	../images/AWS-SecurityGroup-Backend-LoadBalancer.png
[image-11]:	../images/AWS-SecurityGroup-Backend.png
[image-12]:	../images/AWS-SecretManager-Overview.png
[image-13]:	../images/AWS-Compute-EC2.png
[image-14]:	../images/AWS-Compute-AutoScalingGroup.png