# 4. Kubernetes

As we saw in Chapter 8, we built out the Kubernetes deployments using the Terraform provider for Kubernetes. Like Packer and Docker, Kubernetes, in its own way, provides a control plane that operates consistently across cloud platforms. As a result, much of the Kubernetes deployment process is reusable no matter what cloud platform you choose. This is also one of the appeals of Kubernetes as a way to implement cloud agnostic or cloud portable workloads yet leverage the efficiency and elasticity that Kubernetes managed service offerings provide.

In this chapter, we won’t retread the same topics and if you happened to skip Chapters 7 through 9 due to lack of interest in AWS, I’d highly recommend you go back and review the corresponding section in Chapter 8 for more details about the implementation of the Kubernetes deployments.

## Provider Setup

As we saw in Chapter 8, when executing Terraform using the Kubernetes provider to provision resources to the Kubernetes control plane, there are not many changes. We still authenticate against our target cloud platform, we still follow Terraform’s core workflow, and we still pass in additional input parameters for platform-specific resources that we need to reference. Most notably, information about the cluster but also other Azure services like our ACR, KeyVault, the Managed Identity, and other details that might need to be put into Kubernetes ConfigMaps that can be used by the pods to point themselves at the endpoint of their database.

```
data "azurerm_kubernetes_cluster" "main" {
	name                = var.kubernetes_cluster_name
	resource_group_name = var.resource_group_name
}
```

We are using a layered approach to provision the infrastructure first and then provision to Kubernetes. As a result, we can reference the Kubernetes cluster using the data source for a resource that was provisioned by the Terraform workspace that is responsible for the Azure infrastructure. This allows users to access important connectivity details without exporting them outside of Terraform and passing them around during the deployment process.

The above code is a reference to the AKS cluster that was provisioned in the previous deployment stage. Using this reference we can initialize the `kubernetes` provider by using several pieces of data to authenticate with the cluster. 

```
provider "kubernetes" {
	host                   = data.azurerm_kubernetes_cluster.main.kube_admin_config[0].host
	client_key             = base64decode(data.azurerm_kubernetes_cluster.main.kube_admin_config[0].client_key)
	client_certificate     = base64decode(data.azurerm_kubernetes_cluster.main.kube_admin_config[0].client_certificate)
	cluster_ca_certificate = base64decode(data.azurerm_kubernetes_cluster.main.kube_admin_config[0].cluster_ca_certificate)
}
```

The client key is the private key used for authentication, the client certificate is the certificate that is paired with the private key to perform authentication and finally the Cluster’s CA certificate is the certificate of the Certificate Authority that’s used to verify the Kubernetes API Server.

In addition, the `helm` provider can be configured using the same parameters. This can be helpful in providing pre-packaged templates of Kubernetes resources via Helm charts.

```
provider "helm" {
  kubernetes {
    ...
  }
}
```

## Secrets
In the previous section we enabled the KeyVault extension on the cluster itself, now we need to provide a way for the pods to connect to our Azure KeyVault. This requires us to use the Kubernetes Secrets Store Container Storage Interface (CSI) driver. This configuration acts as a conduit, granting the Workload Identity the necessary permissions to read specific secrets from the designated Key Vault.

```
resource "kubernetes_manifest" "secret_provider_class" {
  manifest = {
    apiVersion = "secrets-store.csi.x-k8s.io/v1"
    kind       = "SecretProviderClass"
    metadata = {
      name      = "web-app-secrets"
      namespace = var.namespace
    }
    spec = {
      provider = "azure"
        secretObjects = [
        {
          data = [
            {
              key        = "db-admin-password"
			  objectName = "db-admin-password"
            }
          ]
          secretName = "db-admin-password"
		  type       = "Opaque"
        }
      ]
      parameters = {
        usePodIdentity = "false"
        clientID       = var.service_account_client_id
        keyvaultName   = var.keyvault_name
        cloudName      = ""
        objects = yamlencode([
          {
            objectName    = "db-admin-password"
            objectType    = "secret"
            objectVersion = ""
          }
        ])
        tenantId = var.tenant_id
      }
    }
  }
}
```

In the above code we need to provision this Kubernetes resource into the namespace we plan on deploying our pods and specify both the KeyVault, the Managed Identity which we configured with the Azure Federated Identity Credential and the Kubernetes Service Account. 

## Workload Identity

In order to ensure that our pods use the Managed Identity we need to take a few actions that use both Azure specific schema and standard Kubernetes schema by provisioning resources within Kubernetes and configurations within the Deployment Specifications of our Pods.

The first thing we need to do is to create a Kubernetes Service Account. This is a standard resource within Kubernetes but we use Azure-specific schema to associate it with the Azure Federated Identity Credential. 

```
resource "kubernetes_service_account" "main" {
  metadata {
    namespace = var.namespace
    name      = var.service_account_name
    annotations = {
      "azure.workload.identity/client-id" = var.service_account_client_id
    }
  }
}
```

Using Terraform allows us to substitute dynamic values that are created during the earlier stage of the provisioning process. Kubernetes does have its own way of do things but it involves using Helm and carries with it additional implementation overhead.

Now that the Service Account exists in Kubernetes and it’s linked to the appropriate Azure Managed Identity Credential, the next step is to enable Azure Workload Identity within the deployment. In order to do this we need to specify a special label `azure.workload.identity/use` and set its value to true. 

```
labels = {
	"azure.workload.identity/use" = "true"
}
```

This will inform Azure Kubernetes Service to connect the pods within this deployment to the Managed Identity linked through the Azure Federated Identity Credential.

The next step is to specify the corresponding Kubernetes Service Account that we already linked to the Azure Federated Identity Credential in the previous section. This service account is set on the Pod’s specification within the Deployment.

```
spec {

	...

	service_account_name = "workload"

	...

}
```