# 3. Infrastructure

As we discussed in the previous section, much of the infrastructure is unchanged when using container-based architecture. Therefore, in this section we’ll be focusing on what’s different when use Azure’s Kubernetes managed service.

## Container Registry

The first component we need to provision is our Container Registry. The Container Registry is often provisioned as part of a separate deployment reserved for shared infrastructure that is reused across multiple applications. This can help when you have a common set of custom-built images that multiple teams or projects need to use in their applications or services. However, you should keep in mind that the Container Registry does act as an important security boundary, so if you want to ensure that application teams can only access images built for their applications, you should provision an isolated Container Registry for each project team.

```
resource "azurerm_container_registry" "main" {

  name                    = replace("acr${var.application_name}${var.environment_name}", "-", "")
  resource_group_name     = azurerm_resource_group.main.name
  location                = azurerm_resource_group.main.location
  sku                     = "Premium"
  admin_enabled           = true
  zone_redundancy_enabled = true

}
```

The above code provisions the Azure Container Registry. It’s important to note that this resource has very specific requirements for the name. 

```
resource "azurerm_role_assignment" "acr_push" {

  count = length(var.container_registry_pushers)

  scope                = azurerm_container_registry.main.id
  role_definition_name = "AcrPush"
  principal_id         = var.container_registry_pushers[count.index]

}
```

The above code creates a role assignment that will allow different users to push container images to this container registry. This is a critical requirement to allow our GitHub Action to publish the Docker image we build to our Azure Container Registry. The `principal_id` must be set to the identity of the service account that our GitHub Action impersonates. In this case, I pass in a collection of these and iterate over that collection using the `count` meta-argument. In the case of Role Assignments, because these resources are so lightweight, it doesn’t matter much if we use `for_each` or `count` because the drop-create that will occur more frequently when using `count` has little impact on the deployment.

## Kubernetes Cluster

The next step is to provision a Kubernetes Cluster using the `azurerm_kubernetes_cluster` resource.  This resource will be the central figure in our Azure Kubernetes Service infrastructure.
	
```
resource "azurerm_kubernetes_cluster" "main" {

  name                      = "aks-${var.application_name}-${var.environment_name}"
  location                  = azurerm_resource_group.main.location
  resource_group_name       = azurerm_resource_group.main.name
  dns_prefix                = "${var.application_name}-${var.environment_name}"
  node_resource_group       = "${azurerm_resource_group.main.name}-cluster"
  sku_tier                  = "Standard"

  ...

}
```

The above code configures some important top-level attributes that influence pricing, networking, and internally managed resource placement. Azure Kubernetes Service will provision resources to two resource groups. One is where the AKS resource exists, and the other is where AKS provisions the internal Azure resources that make up the internals of the cluster. This secondary resource group’s name is controlled by the `node_resource_group` attribute. I would always recommend setting the `node_resource_group` name to something that is cohesive with the naming convention of the Azure Kubernetes Service cluster resource itself.

As we learned in Chapter 5, Kubernetes has a number of system services that need to be deployed and in good health for the cluster to function correctly. Our AKS cluster needs to have one or more node pools to hosted system and user workloads. The Default Node Pool is a great place to host these system services.

```
resource "azurerm_kubernetes_cluster" "main" {

  ...

  default_node_pool {
  name                        = "systempool"
  vm_size                     = var.aks_system_pool.vm_size
  enable_auto_scaling         = true
  min_count                   = var.aks_system_pool.min_node_count
  max_count                   = var.aks_system_pool.max_node_count
  vnet_subnet_id              = azurerm_subnet.kubernetes.id
  os_disk_type                = "Ephemeral"
  os_disk_size_gb             = 30
  orchestrator_version        = var.aks_orchestration_version
  temporary_name_for_rotation = "workloadpool"

  zones = [1, 2, 3]

  upgrade_settings {
  	max_surge = "33%"
  }

  ...

}
```

Additional node pools like the one below can be created to allow us to isolate our custom deployments on their own dedicated computing resources so that they don’t impact the day-to-day operations of the cluster.

```
resource "azurerm_kubernetes_cluster_node_pool" "workload" {

  name                  = "workloadpool"
  kubernetes_cluster_id = azurerm_kubernetes_cluster.main.id
  vm_size               = var.aks_workload_pool.vm_size
  enable_auto_scaling   = true
  min_count             = var.aks_workload_pool.min_node_count
  max_count             = var.aks_workload_pool.max_node_count
  vnet_subnet_id        = azurerm_subnet.kubernetes.id
  os_disk_type          = "Ephemeral"
  orchestrator_version  = var.aks_orchestration_version

  mode  = "User" # Define this node pool as a "user" aka workload node pool
  zones = [1, 2, 3]

  upgrade_settings {
    max_surge = "33%"
  }

  node_labels = {
    "role" = "workload"
  }

  node_taints = [              
    "workload=true:NoSchedule"
  ]

}
```

By setting a taint on the nodes within this node pool, we can ensure that only Kubernetes deployments that are explicitly targeted to this node pool will be scheduled here. By employing taints on your additional node pools, you can isolate Kubernetes system services from the default node pool and keep your workloads in their own space. This does have additional costs, but it will greatly improve the health and performance of the cluster. It is definitely something you should do if you are planning on deploying production workloads to your cluster—but if you are just kicking the tires, feel free to skip it!

## Identity & Access Management
Managed Identity plays an integral role in the configuration of an Azure Kubernetes Service in several different ways. The first and most important is the Managed Identity that Azure Kubernetes Service will use to provision the internal resources.

```
resource "azurerm_kubernetes_cluster" "main" {

  ...

  identity {
    type         = "UserAssigned"
    identity_ids = [azurerm_user_assigned_identity.cluster.id]
  }

  ...

}
```

This identity needs to be assigned the “Managed Identity Operator” role in order to perform this function.

```
resource "azurerm_role_assignment" "cluster_identity_operator" {

  scope                = azurerm_resource_group.main.id
  role_definition_name = "Managed Identity Operator"
  principal_id         = azurerm_user_assigned_identity.cluster.principal_id

}
```

The above code creates this role assignment using a User Assigned Managed Identity. We’ve explored this topic in the previous chapter so we know that this is a special type of Managed Identity that we explicitly provision and assign role assignments to. This is in contrast to the System Assigned Identity which is a Managed Identity that is automatically provisioned and managed by the platform itself.

There is another important identity that needs to be set on the AKS cluster, that is the Managed Identity used by the kubelet system service deployed to each node within the cluster. 

```
resource "azurerm_kubernetes_cluster" "main" {

  ...

  kubelet_identity {
    client_id                 = azurerm_user_assigned_identity.cluster_kubelet.client_id
    object_id                 = azurerm_user_assigned_identity.cluster_kubelet.principal_id
    user_assigned_identity_id = azurerm_user_assigned_identity.cluster_kubelet.id
  }

}
```

The above code below configures the cluster’s kublet identity. This is a little inconsistent than how Managed Identities are typically attached within the `azurerm` provider so it’s important to get the correct outputs from the User Assigned Identity to the right attributes of the `kubelet_identity` block.

As we learned in Chapter 5, the kubelet system service processes orders from the Scheduler. In order to do this, the kubelet will need access to pull container images from our ACR. This will require the “AcrPull” Role Assignment to be added to the above Managed Identity.

```
resource "azurerm_role_assignment" "cluster_kubelet_acr" {

  principal_id         = azurerm_user_assigned_identity.cluster_kubelet.principal_id
  role_definition_name = "AcrPull"
  scope                = azurerm_container_registry.main.id

}
```

## Secrets Management

In order to integrate with Azure’s secret management service, KeyVault, we need to take a couple of steps. The first is to simply enable the subsystem on the cluster itself. Azure Kubernetes Service has an extensible model for such features—including but not limited to enabling integrations with other Azure Services and Kubernetes features such as KEDA (Kubernetes Event Driven Architecture), Azure Monitor, and Open Service Mesh. 

```
resource "azurerm_kubernetes_cluster" "main" {

  ...

  key_vault_secrets_provider {
    secret_rotation_enabled  = true
    secret_rotation_interval = "5m"
  }

  ...

}
```

The above code enables and configures secret rotation. This is just the first step to enabling AKS integration with Azure KeyVault, we also have to setup the CSI provider for pods to pull secrets from KeyVault but we’ll look at that in the next section when we start provisioning things to the Kubernetes control plane.

## Workload Identity

In order to allow our pods to access other resources deployed to Azure we need to enable them to impersonate an Azure Managed Identity. Like the integration with KeyVault, we first need to enable this extension on the AKS cluster.

```
resource "azurerm_kubernetes_cluster" "main" {

  ...

  oidc_issuer_enabled       = true
  workload_identity_enabled = true

  ...

}
```

The above code activates an internal OpenID Connect (OIDC) endpoint that is used to sign and issue JSON Web Tokens (JWTs) for the service accounts within the cluster. After this is enabled, we’ll also need Azure Federated Identity Credential, which, once linked to the AKS cluster’s OIDC issuer endpoint and the Managed Identity to be used by the workloads, creates federation between the cluster and Microsoft Entra ID—allowing the pods using the corresponding Kubernetes Service Account to interact with Azure services using the privileges of the Managed Identity.

```
resource "azurerm_federated_identity_credential" "main" {

  name                = azurerm_user_assigned_identity.workload.name
  resource_group_name = azurerm_resource_group.main.name
  audience            = ["api://AzureADTokenExchange"]
  issuer              = azurerm_kubernetes_cluster.main.oidc_issuer_url
  parent_id           = azurerm_user_assigned_identity.workload.id
  subject             = "system:serviceaccount:${var.k8s_namespace}:${var.k8s_service_account_name}"
	
}
```
Just as we did in Chapter 8 when working with AWS, we'll link this to a Kubernetes service account in the next section when we provision resources to Kubernetes.