# 2. Cloud Architecture

There will be many similarities between the work that we did in Chapter 8 when we performed a similar transition from Virtual Machines to containers using the Amazon Web Services platform. We’ll try and focus only on the key differences and avoid retreading the same ground. In order to obtain a complete and multi-cloud perspective, I’d encourage you to read Chapter 8 (in case you skipped it) as well as the upcoming chapter, where we tackle the same problem on the Google Cloud Platform.

## Virtual Network
In the previous chapter, we set up a Virtual Network for two distinct groups of Virtual Machines, and then we connected our application to a database-managed service. When setting up a Virtual Network for a Kubernetes cluster, we’ll use a similar approach. However, the considerations are slightly different. We no longer have distinct and loose virtual machines where we host different components of our application. However, depending on the configuration of our Kubernetes cluster, we may need to consider the placement of the different node pools that you configure and other services that you want to provision within that network to allow the workloads you host on Kubernetes will need access to.

![Resource][image-1]

_With AKS solutions Virtual Network subnets are organized along infrastructure boundaries rather than application boundaries_

In its simplest form, a single subnet can be designated for all the node pools within an AKS cluster, but this can be very limiting as your workload needs to scale up over time. For more advanced scenarios, you should carefully consider the segmentation of your subnets based on your Node Pool design and scale considerations for each of your workloads. In doing so, you can provide better network isolation for the various workloads you host on the cluster.

As we saw when working with Amazon’s Kubernetes offering in Chapter 8, Azure’s Kubernetes offering also supports two networking modes: Kubenet and CNI. For the purposes of this book we’ll again be focusing on Kubenet as its the most commonly used option.

## Container Registry
Just as we saw with Amazon Web Services, Azure has a robust Container Registry service known as Azure Container Registry (ACR). It acts as a private registry for hosting and managing your container images and Helm charts. As we did in the expedition along the Amazon, we’ll be using Docker to publish our own container images to this repository so that we can reference them later from the Terraform that provisions resources to our Azure Kubernetes Service Cluster. We’ll need to grant our cluster access using Azure Managed Identity and Azure’s Role-Based Access Control (RBAC), which is similar to how we granted access to Amazon Elastic Kubernetes Service using AWS’s Identity & Access Management service policies.

## Load Balancing
One of the biggest advantages of hosting your container-based workloads using a Kubernetes-managed service is that much of the underlying infrastructure is automatically configured and maintained on your behalf. The service interprets your Kubernetes resource configuration and provisions the necessary resources within the cluster to properly configure Azure to support your workloads. Sometimes, this is handled transparently, and other times, there are special hooks that allow you more control over the configuration of the underlying resources on Azure.

In this manner, under the hood, Azure Kubernetes Service (AKS) streamlines load balancing using either a basic Azure Load Balancer or a more feature-rich Azure Application Gateway. AKS manages the creation and configuration of these load balancers when services of type `LoadBalancer` are created within the Kubernetes cluster. For more control, users can also utilize Ingress controllers like NGINX or the Azure Application Gateway Ingress Controller (AGIC) for advanced routing, SSL termination, and other capabilities. 

![Resource][image-2]

_Network Traffic Flow of an Azure Kubernetes Service (AKS) Cluster_

As we saw in Chapter 8 when working with Amazon Web Services, we will be using the NGINX ingress controller but this time we'll be provisioning an Azure Application Gateway service to route traffic to NGINX. This works a bit differently than on AWS, where the NGINX ingress controller automatically configured the ALB through Kubernetes annotation. With Azure, we need to setup the NGINX ingress controller and then provision Application Gateway and configure it to forward traffic to NGINX.

## Network Security
In Azure Kubernetes Service (AKS), network security is managed in a manner akin to the practices described in Chapter 10 for Virtual Machines, as they are deployed within Azure Virtual Networks, thus allowing them to integrate seamlessly with existing Azure networking features. However, because Kubernetes has its own overlay network called Kubenet, which is the network on which our workloads (or pods) live, we need to use Kubernetes Network Policies to control network traffic between our workloads based on Kubernetes tags or namespaces. There are definitely more advanced networking security capabilities when you are working with Azure CNI and other open-source solutions like Calico, but these are beyond the scope of this book.

## Secrets Management

Just as we saw on our tour down the Amazon, Azure’s Kubernetes offering also integrates with other Azure services, such as Azure’s secret management service, Azure KeyVault. This integration is done through a combination of an Azure Kubernetes Services extension being enabled on the cluster itself and Kubernetes resources that are provisioned within the cluster, creating Kubernetes resources that our pods can use as a conduit to the secrets hosted on Azure KeyVault. Again, there is nothing stopping us from using native Kubernetes Secrets, but Azure KeyVault provides a much more streamlined and secure mechanism for granting Azure secrets. It allows us to keep secrets up-to-date to avoid outages when secret rotations occur, and it allows us to use Managed Identity to access the secrets rather than storing them on the cluster itself.

Just as we saw in Chapter 8 when building our solution with AWS Elastic Kubernetes Service (EKS) we need to facilitate a bridge between Kubernetes and the cloud platform's Identity Management system. On AWS, that was IAM; on Azure, that's Entra ID. The process is largely the same but the terminology is different.

![Resource][image-4]

_Azure Kubernetes Service (AKS) with Workload Identity_

First we need to create a Managed Identity which will represent the workload. This is an Azure resource that represents an Entra ID identity that is managed by the Azure platform. Like we did with EKS, we need to federate between the Kubernetes cluster and Entra ID. On Azure, we do that by creating a Federated Identity Credential linking the Managed Identity, the AKS cluster's internal Open ID Connect provider and Entra ID. Like on AWS, we plant a seed for this Managed Identity to be linked to a Kubernetes Service Account resource that will be provisioned later within Kubernetes.

![Resource][image-5]

_Azure Kubernetes Service (AKS) Secrets Manager Integration_

After Workload Identity has been established, we can grant access to Azure resources like KeyVault and Databases such as Azure Cosmos DB or Azure SQL Database. Just as we did in Chapter 8 with EKS, we'll use the Secrets Store CSI Driver and the Azure provider to integrate our Kubernetes deployments with Azure KeyVault.

## Kubernetes Cluster
Finally, creating a Kubernetes cluster using Azure Kubernetes Service (AKS) involves a few critical components. As we’ve established, we need a Virtual Network, Managed Identities, and sufficient Role-Based Access Controls to access the resources our cluster needs, such as Container Registries and Azure KeyVault secrets—but the main components of our Kubernetes Cluster are the Node Pools that provide compute resources to host our pods. 

![Resource][image-3]

_Anatomy of an Azure Kubernetes Service (AKS) Cluster_

By default, every AKS cluster comes with a default node pool, which is where Kubernetes’ system services are hosted, but we can add additional node pools either to isolate our application workloads or to grant access to different types of computing resources, such as different hardware profiles to meet the specific needs of different workloads.




[image-2]:	../images/Azure-AKS-AppGateway-NetworkFlow.png
[image-1]:	../images/Azure-VirtualNetwork.png
[image-3]:	../images/Azure-AKS-Cluster.png
[image-4]:	../images/Azure-AKS-WorkloadIdentity.png
[image-5]:	../images/Azure-AKS-KeyVault.png