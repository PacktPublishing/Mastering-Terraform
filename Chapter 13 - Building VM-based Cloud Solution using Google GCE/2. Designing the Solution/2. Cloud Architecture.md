# 2. Cloud Architecture

The first part of our design is adapting our solution’s architecture to the target cloud platform: Google Cloud. This involves mapping application architecture components to Google Cloud Platform services and thinking through the configuration of those services to meet the requirements of our solution.

## Projects & API Access

Before we get started, we need a project within the organization where a service account can be created for Terraform to use. This service account needs to be granted access to the following organizational role: `roles/resourcemanager.projectCreator`. This will allow you to create projects with Terraform, which will allow you to keep a complete solution together and avoid additional boilerplate prerequisites executed outside of Terraform using the command line interface.

Once this has been done you need to enable the “Cloud Resource Manager API” within the project that the Terraform Service Account resides. This API is required within the context of the Google Cloud Project because of the way Google Cloud grants access to different features of the platform at the project-level. It creates another gate for the Google Cloud identity to be able to access resources on the Google Cloud Platform.

Your Terraform Service account will also need access to the Cloud Storage, which you plan on using to store Terraform State. When using the AWS and Azure providers, you can use different credentials to access the Terraform Backend than you use to provision your environment. On Google Cloud, this is accomplished by setting `GOOGLE_BACKEND_CREDENTIALS` with credentials for the identity you wish to use to communicate with the Google Cloud Storage Bucket and `GOOGLE_APPLICATION_CREDENTIALS` with credentials for the identity you wish to use to communicate with Google Cloud to provision your environment.

## Virtual Network

Virtual Machines must be deployed within a Virtual Network. If you recall from Chapter 7 when we provisioned this solution on AWS we needed to setup multiple Subnets in order for our solution to span Availability Zones. In Chapter 8, when deploying the solution to Azure we only needed two Subnets—one for the Frontend and one for the Backend. That’s because Azure’s Virtual Network architecture is structured differently than AWS’s and Subnets on Azure span multiple Availability Zones. 

Google Cloud’s Virtual Network Service is also structured differently. Unlike both AWS and Azure which have Virtual Networks scoped to a particular Region, Virtual Networks on GCP span multiple Regions by default. Subnets are scoped to the Region which means, like Azure, a Subnet on GCP can host Virtual Machines from multiple Availability Zones.

![Resource][image-1]
_Google Cloud Network Architecture_

In the above diagram you can see that the Google Compute Network is not tied to the Region like it is on AWS and Azure. Although this seems like a significant difference at the root of the deployment hierarchy, it doesn’t materially impact the design as the Subnets (or “Subnetworks”) are still tied to a Region.

![Resource][image-2]
_Isolated Subnets for Frontend and Backend Application Components_

When building a single-region solution, the multi-region capability of Google Cloud might seem like overkill but the automatic spanning does simplify infrastructure management, as businesses don't have to manually set up and maintain inter-regional connections. This not only reduces administrative overhead but also allows for more agile and scalable deployments in response to changing demands by making active-active multi-region deployments easier to build and maintain.

## Network Routing

Inside Google Cloud Networks, the default setup is designed to provide straightforward and secure connectivity. As we know, by default, Google Cloud Networks are global resources, meaning all the Subnets (or “subnetworks”) within a single Network can communicate with each other, regardless of their regional location, without the need for explicit routes or VPNs. This inter-subnet communication uses the system-generated routes in the Network. 

For routing configurations, Google Cloud has "Routes,” which perform a role similar to AWS's Route Tables, directing traffic based on IP ranges. For situations where instances need to initiate outbound connections to the internet without revealing their IP, Google Cloud provides Cloud NAT, which is analogous to AWS's NAT Gateways. 

Like Azure, Google Cloud does not have a direct equivalent named "Internet Gateway." Instead, internet connectivity in GCP is managed using a combination of system-generated routes and firewall rules. 

## Load Balancing

Google Cloud has two options when it comes to Load Balancers: Global and Regional. Global Load Balancers distribute traffic across multiple regions, ensuring users are served from the nearest or most suitable region, while Regional Load Balancers distribute traffic within a single region. The choice between them typically depends on the application's user distribution and the need for low-latency access. However, sometimes, there are other limitations that force your hand.

![Resource][image-3]
_Google Cloud Regional Load Balancer_

Unfortunately, the Regional Load Balancer's target pool does not allow you to specify a different port for the backend instances. This means the target pool will forward traffic to the same port where it received traffic. For instance, if the forwarding rule is listening on port 80, the target pool will send traffic to port 80 of the backend instances.

To achieve your goal of forwarding from port 80 to port 5000, you would need to use the Global Load Balancer instead of the Regional Load Balancer.

![Resource][image-4]
_Google Cloud Global Load Balancer_

The Global Load Balancer requires that you set up Instance Groups to organize the Virtual Machines that the load will be distributed across. Google Cloud Instance Groups are similar to AWS Auto-Scaling Groups and Azure's Virtual Machine Scale Sets, but they have a bit more flexibility in that you can either provide a Virtual Machine template and allow the Google Cloud platform to "manage" the instances or you can provision the instances explicitly and add them later to the Instance Group. This dual-mode capability is similar to Azure’s Virtual Machine Scale Set rather than AWS Auto-Scaling Group, which can only operate in a "managed" mode.

Just as we saw when comparing AWS and Azure, all the anatomical parts of a Load Balancer are present and accounted for they just might go by different names and connect together in slightly different ways. The below table extends the mapping that we did between AWS and Azure and includes the Google Cloud Platform equivalents.

| AWS                             | Azure                     | GCP                    | Description                                                                                                            |
| ------------------------------- | ------------------------- | ---------------------- | ---------------------------------------------------------------------------------------------------------------------- |
| Application Load Balancer (ALB) | Azure Load Balancer       | URL Map                | Load Balancer                                                                                                          |
| Listener                        | Frontend IP Configuration | Global Forwarding Rule | The singular endpoint that accepts incoming traffic on a Load Balancer                                                 |
| Target Group                    | Backend Address Pool      | Backend Service        | A collection of Virtual Machines that incoming traffic is forwarded to                                                 |
| Health Check                    | Health Probe              | Health Check           | An endpoint published by each of the backend Virtual Machines that indicates it is healthy and ready to handle traffic |

_Mapping of synonymous Load Balancer components between AWS, Azure, and Google Cloud Platform_

The URL Map and the Target HTTP Proxy together compose the Global Load Balancer, which attaches to the Forwarding Rule, which acts as the singular endpoint, and the Backend Service, which represents the collection of Virtual Machines to distribute load across.

## Network Security

To control network traffic, Google Cloud offers firewall rules that allow users to specify which packets are allowed into and out of instances. While Google Cloud's firewall rules share some similarities with AWS's Network Access Control Lists (NACLs), it's crucial to note that GCP firewall rules are stateful, while AWS NACLs are stateless. 

## Secrets Management

Secrets such as database credentials or service access keys need to be stored securely. Each cloud platform has its own service that provides this functionality. On GCP, this service is called Google Cloud Secret Manager.

Again, we will see slight naming convention differences but all the anatomical parts are there. The below table extends the mapping that we did between AWS and Azure and includes the Google Cloud Platform equivalents.

| AWS             | Azure                            | GCP             | Description                                                                  |
| --------------- | -------------------------------- | --------------- | ---------------------------------------------------------------------------- |
| IAM             | Microsoft Entra                  | Cloud Identity  | Identity Provider                                                            |
| Secrets Manager | KeyVault                         | Secret Manager  | Secure secret storage                                                        |
| IAM Role        | User Assigned Managed Identity   | Service Account | Identity for machine to machine interaction                                  |
| IAM Policy      | Role Based Access Control (RBAC) | IAM Member      | Permissions to perform specific operations on specific services or resources |
| IAM Role Policy | Role Assignment                  | IAM Member      | Association of specific permissions to specific identites                    |

_Mapping of synonymous Identity & Access Management components between AWS, Azure, and Google Cloud Platform_

Secrets stored in Google Cloud Secret Manager can be accessed by Virtual Machines once they have the necessary access granted. In Chapter 7, we used an AWS IAM Role assignment to allow a Virtual Machine to do this, and with Azure, we used a User Assigned Managed Identities and Role Assignments. On GCP, we need to use a Service Account and grant it permissions to the specific secrets.

![Resource][image-5]
_KeyVault Architecture_

Granting the Managed Identity that is attached to the Virtual Machines access to the “Key Vault Secrets User” Role will allow the Virtual Machines to read the secret values from Key Vault. This does not put the secrets on the machine. The Virtual Machine will need to use the Azure CLI to access the KeyVault secrets.

## Virtual Machines

Now that we have everything we need for our solution, we can finish by talking about where our application components will actually run: Virtual Machines provisioned on Google Cloud's Compute Engine service. When provisioning Virtual Machines on GCP, you have two options. First, you can provide static virtual machines. In this approach, you need to specify key characteristics for every Virtual Machine. You can organize these Virtual Machines into an Instance Group to better manage the health and lifecycle of the Virtual Machines. The second option is to provision an Instance Group Manager. This will allow you to dynamically scale up and down based on demand as well as auto-heal Virtual Machines that fail.

![Resource][image-6]
_Google Cloud Compute Engine Instance Architecture_

Similarly to Azure, Google Cloud separates the concept of grouping Virtual Machines that are tied together through application lifecycle from the management of their health and dynamic provisioning of them. In Azure, the Availability Set is a logical group that can be used to place individual Virtual Machines into a relationship so that their relationship is taken into consideration by the underlying platform. 

![Resource][image-7]
_Instance Group Manager Architecture_

On Google Cloud, that is an Instance Group. Both allow you to easily attach a pool of Virtual Machines to other services relevant to multiple Virtual Machines working a problem together such as Load Balancers and Health Monitoring. To add dynamic provisioning and management, on Azure you would need a Virtual Machine Scale Set, on Google Cloud this is called an Instance Group Manager.

Again, just as we saw previously, the names have been changed to protect the innocent, but make no mistake, they work the same way. The table below extends the mapping that we did between AWS and Azure and includes the Google Cloud Platform equivalents.

| AWS                      | Azure                            | GCP                    | Description                                                                                                               |
| ------------------------ | -------------------------------- | ---------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| EC2                      | Virtual Machines                 | Compute Instance       | Virtual Machine Service                                                                                                   |
| AMI                      | Virtual Machine Image            | Google Compute Image   | Virtual Machine Image either from Marketplace or custom build (e.g. using tools like Packer)                              |
| IAM Role                 | User Assigned Managed Identity   | Service Account        | Identity for machine to machine interaction                                                                               |
| Auto-Scaling Group (ASG) | Virtual Machine Scale Set (VMSS) | Instance Group Manager | Set of dynamically provisioned Virtual Machines that can be scaled up/down using a Virtual Machine configuration template |
| Launch Template          | Virtual Machine Profile          | Instance Template      | Configuration template used to create new Virtual Machines                                                                |

_Mapping of synonymous Virtual Machine service components between AWS and Azure_

In Chapter 7 we provisioned our solution using AWS Elastic Cloud Compute (EC2) service and in Chapter 8, we did the same but with the Azure Virtual Machine service. Like both of these platforms, on GCP, Virtual Machines are connected to Virtual Networks using Network Interfaces. Unlike AWS and Azure, these network interfaces cannot be provisioned independently of the virtual machine and are then attached later. 

We also discussed the subtle differences between how Azure and AWS handle network security, with AWS having low-level network security handled by Network Access Control Lists (NACLs) that attach at the subnet and more logical Security Groups that attach at the instance and process network traffic in a stateful manner. Azure has similar constructs with Network Security Groups, which focus more on network traffic between physical endpoints (IP address ranges and network gateways), and Application Security Groups, which focus on network traffic between logical application endpoints. Google Cloud combines the two into Google Compute Firewall resources that can control network traffic using physical network characteristics such as IP address ranges and logical constructs such as Service Accounts and tags.

This pattern of using tags to attach behavior or grant permissions is a common pattern on Google Cloud Platform and you should make note of it as other platforms do not regard tags as a method for establishing security boundaries.

[image-1]:	../images/GCP-VirtualNetwork.png
[image-2]:	../images/GCP-VirtualNetwork-FrontendBackend.png
[image-3]:	../images/GCP-LoadBalancer-Simple.png
[image-4]:	../images/GCP-LoadBalancer-Complex.png
[image-5]:	../images/GCP-SecretManager-Overview.png
[image-6]:	../images/GCP-Compute-VirtualMachines.png
[image-7]:	../images/GCP-Compute-InstanceGroup.png