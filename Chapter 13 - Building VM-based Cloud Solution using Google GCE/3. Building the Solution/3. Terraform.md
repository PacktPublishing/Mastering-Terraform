# 3. Terraform

As we discussed in our design, our solution is made up of two Application Components: the front end and the back end. Each has its own codebase of Application Code that needs to be deployed. Since this is the first time we will be using the `google` provider, we’ll look at basic provider setup and the configuration of the backend before we move into the nuts and bolts of each component of our architecture.

## Provider Setup
We need to specify all the providers that we intend to use in this solution within the `required_providers` block.

	terraform {
	  required_providers {
	    google = {
	      source  = "hashicorp/google"
	      version = "~> 5.1.0"
	    }
	    cloudinit = {
	      source  = "hashicorp/cloudinit"
	      version = "~> 2.3.2"
	    }
	    random = {
	      source  = "hashicorp/random"
	      version = "~> 3.5.1"
	    }
	  }
	  backend "gcs" {
	  }
	}

We’ll also configure the Google Cloud provider. The Google Cloud provider, like Azure but unlike the AWS provider, is not scoped to a particular region. The Google Cloud provider doesn't even need to be scoped to a project. In this way, it is extremely flexible and can be used to provision cross-project and multi-region resources with the same provider declaration.

	provider "google" {
	  project = var.gcp_project
	  region  = var.primary_region
	}

One major difference between the Google Provider and the AWS and Azure providers is how you authenticate. While Azure and AWS have environment variables that specify the identity, the Google Cloud provider relies on an authentication file, so this will alter how our pipeline tools integrate with Terraform to ensure a Google Cloud solution has the right identity. The environment variable `GOOGLE_APPLICATION_CREDENTIALS` specifies the path to this file. It is important to note that this file is a JSON file, but it contains secret information; therefore, it should be treated as a credential and protected as such.

## Backend

Because we will be using a CI/CD pipeline to provision and maintain our environment long term we need to setup a remote backend for our Terraform State. Because our solution will be hosted on Google Cloud, we’ll use the Google Cloud Storage backend to store our Terraform State.

Just like the Google Cloud provider, we don’t want to hard code the backend configuration in our code so we simply setup a placeholder for the backend.

	terraform {
	
	  ...
	
	  backend "gcs" {
	  }
	}

We’ll configure the backend’s parameters using the `-backend-config` parameters when we run `terraform init` in our CI/CD pipeline.

## Input Variables

It’s good practice to pass in short names that identify the application’s name and the application’s environment. This allows you to embed consistent naming conventions across the resources that make up your solution, which makes it easier to identify and track resources from the Google Cloud Console.

The `primary_region`, `network_cidr_block` and `az_count` input variables drive key architectural characteristics of the deployment. It is important that they are not hard-coded as it would limit the reusability of the Terraform codebase. 

The `network_cidr_block` establishes the Virtual Network address space, which is often tightly regulated by an enterprise governance body. There is usually a process to ensure that teams across an organization do not use IP address ranges that conflict, thus making it impossible in the future to allow those two applications to integrate with one another or integrate with shared network resources within the enterprise.

The `az_count` allows for configurability of how much redundancy we want within our solution. This will affect the high availability of the solution but also the cost of the deployment. As you can imagine, cost, is also a tightly regulated characteristic of cloud infrastructure deployments.

## Consistent Naming & Tagging

Unlike the AWS Console, and very similar to Azure, Google Cloud is designed in such a way that it is extremely easy to get an application-centric view of your deployment: Projects! Therefore, it's not as important as an organizational strategy for your application to specify tags. You will, by default, have a project-centric view of all resources on Google Cloud.

	resource "google_compute_network" "main" {
	
	  ...
	
	  tags = {
	    application = var.application_name
	    environment = var.environment_name
	  }
	}

It’s still important to tag the resources that you deploy that indicate what application and what environment they belong to. This helps for other reporting needs like budgets and compliance. Almost all resources within the Google Cloud provider have a `map` attribute called `tags`. Like Azure, each resource has usually has `name` as a required attribute. 

## Virtual Network

Just as we did in Chapters 7 and 8, we need to construct a Virtual Network and keep its address space as tight as possible to avoid gobbling up unnecessary address space for the broader organization in the future.

	resource "google_compute_network" "main" {
	  name                    = "${var.application_name}-${var.environment_name}"
	  auto_create_subnetworks = false
	}

The network creation in Google Cloud is simpler than what we did with AWS because we don’t have to segment our subnets based on Availability Zone. This approach resembles how Azure structures subnets to span Availability Zones.

	resource "google_compute_subnetwork" "frontend" {
	  name          = "frontend"
	  region        = var.primary_region
	  network       = google_compute_network.main.self_link
	  ip_cidr_range = cidrsubnet(var.network_cidr_block, 2, 1)
	}

## Load Balancing

As we discussed in the design, the Google Cloud Load Balancer service is structured quite a bit differently than AWS and Azure's equivalent offerings.

The Global Forwarding Rule acts as the main entry point for the Global Load Balancer.
	resource "google_compute_global_forwarding_rule" "frontend" {
	  name        = "my-forwarding-rule"
	  ip_protocol = "TCP"
	  port_range  = "80"
	  target      = google_compute_target_http_proxy.http_proxy.self_link
	}

It then references a Target HTTP Proxy.

	resource "google_compute_target_http_proxy" "http_proxy" {
	  name    = "my-http-proxy"
	  url_map = google_compute_url_map.url_map.self_link
	}

Which subsequently references a URL Map.

	resource "google_compute_url_map" "url_map" {
	  name            = "my-url-map"
	  default_service = google_compute_backend_service.backend_service.self_link
	}

The URL Map points to a Backend Service which ultimate defines which Google Cloud Services will be handling the requests.

	resource "google_compute_backend_service" "backend_service" {
	  name        = "my-backend-service"
	  port_name   = "http"
	  protocol    = "HTTP"
	  timeout_sec = 10
	
	  dynamic "backend" {
	    for_each = google_compute_instance_group.frontend
	    content {
	      group = backend.value.self_link
	    }
	  }
	
	  health_checks = [google_compute_http_health_check.frontend.self_link]
	}

In the above code you can see that we are connecting the backend to both a Health Check and the Instance Group that contains the Virtual Machines that will ultimately be handling the incoming requests.

	resource "google_compute_http_health_check" "frontend" {
	  name = "${var.application_name}-${var.environment_name}-hc"
	
	  port         = 5000
	  request_path = "/"
	}

The Health Check provides the configuration for the platform to determine if the Backend Service is healthy or not with requests being sent to the Health Check endpoint on the corresponding Backend Service to determine if it is healthy enough to receive incoming traffic.

## Network Security

First we need to setup the logical Firewall for each Application Architectural component. We’ll have one for the Frontend and one for the Backend.

	resource "google_compute_firewall" "default-hc-fw" {
	
	  name    = "${var.application_name}-${var.environment_name}-hc"
	  network = google_compute_network.main.self_link
	
	  allow {
	    protocol = "tcp"
	    ports    = [5000]
	  }
	
	  source_ranges = ["130.211.0.0/22", "35.191.0.0/16"]
	  target_tags   = ["allow-lb-service"]
	}

Google Cloud often has specific well-known IP Addresses that need to be included in your Firewall Rules in order to grant the necessary permissions to communicate between services.

## Secrets Management

In Chapter 7, we setup secrets using AWS Secrets Manager and in Chapter 8 we did the same with KeyVault on Microsoft Azure. As you might remember from Chapter 8, Azure KeyVault is provisioned within a region and within this context secrets can be created. Google Cloud’s Secret Manager service works similarly to AWS in that there is no logical endpoint that needs to be provisioned where secrets are scoped within. 

	resource "google_secret_manager_secret" "db_password" {
	  secret_id = "db-password-secret"
	
	  replication {
	    automatic = true
	  }
	}

The above code shows how to provision a Secret within Google Cloud Secret Manager. This is a logical container for a Secret that may have many different values over its lifecycle as a result from secret rotation on a regular basis.

	resource "google_secret_manager_secret_version" "db_password_version" {
	  secret      = google_secret_manager_secret.db_password.id
	  secret_data = "abc1234"
	}

The above code shows how we can define a specific version of the secret. This might be a value that we pull in from other Google Cloud resources that we provision.

	resource "google_secret_manager_secret_iam_member" "secret_iam" {
	  secret_id = "YOUR_SECRET_ID"
	  role      = "roles/secretmanager.secretAccessor"
	
	  member = "serviceAccount:YOUR_SERVICE_ACCOUNT_EMAIL"
	}

The above code grants a Service account access to our Secrets within Google Cloud Secret Manager.

## Virtual Machines

When provisioning static Virtual Machines, we have much more control over the configuration of each machine. Some Virtual Machines have specific network and storage configurations to meet workload demands. 

First, we’ll obtain the Virtual Machine Image from our input variables. This is the Virtual Machine Image that we built with Packer and provisioned into a different Google Cloud Project.

	data "google_compute_image" "frontend" {
	  name = var.frontend_image_name
	}

Then we’ll create a Virtual Machine using the Google Cloud Instance. This resource will contain the Network Interface, Disks, and Service Account configuration to setup our Virtual Machine and connect it to the right Subnetwork in our Virtual Network.

	resource "google_compute_instance" "frontend" {
	
	  count = var.frontend_instance_count
	
	  name         = "vm${var.application_name}-${var.environment_name}-frontend-${count.index}"
	  machine_type = var.frontend_machine_type
	  zone         = local.azs_random[count.index % 2]
	
	  boot_disk {
	    initialize_params {
	      image = data.google_compute_image.frontend.self_link
	    }
	  }
	
	  // Local SSD disk
	  scratch_disk {
	    interface = "NVME"
	  }
	
	  network_interface {
	    subnetwork = google_compute_subnetwork.frontend.self_link
	
	    access_config {
	      // Ephemeral public IP
	    }
	  }
	
	  service_account {
	    # Google recommends custom service accounts that have cloud-platform scope and permissions granted via IAM Roles.
	    email  = google_service_account.main.email
	    scopes = ["cloud-platform"]
	  }
	
	  tags = ["ssh-access", "allow-lb-service"]
	
	}

Then, we’ll create the Network Interface for each Virtual Machine by iterating over the `var.az_count` input variable.

	locals {
	  zone_instances = { for z in local.azs_random : z =>
	    {
	      instances = flatten([
	        for i in google_compute_instance.frontend :
	        i.zone == z ? [i.self_link] : []
	      ])
	    }
	  }
	}

Setup Instance Groups for each Zone:

	resource "google_compute_instance_group" "frontend" {
	
	  count = var.az_count
	
	  named_port {
	    name = "http"
	    port = 5000
	  }
	
	  name      = "frontend-${count.index}"
	  zone      = local.azs_random[count.index]
	  instances = local.zone_instances[local.azs_random[count.index]].instances
	}

Finally, we’ll setup the Virtual Machine with all the necessary attributes set, linking it to the Network Interface, the Virtual Machine Image and the Managed Identity.