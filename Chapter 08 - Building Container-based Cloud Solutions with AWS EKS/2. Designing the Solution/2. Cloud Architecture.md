# 2. Cloud Architecture

In the previous chapter, our cloud-hosting solution was a set of dedicated Virtual Machines. In this chapter, our objective is to leverage AWS Elastic Kubernetes Service (EKS) to use a shared pool of Virtual Machines that are managed by Kubernetes to host our application. In order to achieve this, weâ€™ll be using some new resources that are geared towards container-based workloads but much of the networking, load balancing and other components will largely be the same.

## Virtual Network
Recalling our work in Chapter 7 with EC2 instances and Virtual Networks, setting up a Virtual Private Cloud (VPC) for AWS EKS (Elastic Kubernetes Service) follows a similar process. The core network is still there, with all the pomp and circumstance, from Subnets--both public and private--to all the minutia of route tables, Internet Gateways and NAT Gateways, the Virtual Network we build for our EKS cluster will largely be the same as the one we created previously. The only difference is how we use it.

![Resource][image-1]

_AWS Virtual Network Architecture_

Previously, we used the public subnets for our front end Virtual Machines and the private subnets for our back end. As we learned in Chapter 5, when we introduce Kubernetes into the mix, we'll be transitioning to a shared pool of Virtual Machines that host our application as pods. These virtual machines will be hosted in the private subnets and a load balancer will be hosted in the public subnets.

## Container Registry
Building on our exploration of Container Architecture in Chapter 5, we know that we need to build container images and we need to store them in a container registry. For that purpose, AWS offers Elastic Container Registry (ECR). This is a private container registry, unlike public registries like Docker Hub that we looked at in Chapter 5.

We'll need to utilizing the Docker command-line utility to build and push images to ECR. In order to be able to do that, we need to grant an identity the permissions to do that. As we saw in the previous chapter, when we built virtual machine images using Packer, we'll likely have a GitHub Actions workflow that builds and pushes the container images to ECR. The identity that the GitHub Action workflow executes under will need permissions to do that. These Docker images, once in ECR, the final step is to grant our cluster access to pull images from the registry. 

![Resource][image-2]

_IAM Policy allowing a group access to Push Container Images to ECR_

We'll setup an IAM Group that we'll grant this permission to. This will allow us to add the user for the GitHub Action as well as any other human users that want to push images directly from the command line. In AWS, IAM policies are extremely flexible, they can be declared independently or inline with the identity they are being attached to. This allows us to create reusable policies that can be attached to multiple identities. In this case, we'll define the policy that grants access to push images to this ECR and then attach it to the Group. Then membership in the group will grant users access to these permissions.

The final step is to grant access to the cluster such that it can pull images from our ECR when it schedules pods within the nodes. In order to do that we can use a built-in AWS policy called `AmazonEC2ContainerRegistryReadOnly`. We'll need to reference it using its fully qualified ARN which looks like this: `arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly`. Built-in policies have a common `arn:aws:iam::aws:policy` prefix that identifies them as published by AWS and not published by any specific user within their AWS account. When we publish our own policies the fully qualified ARN will include our account number.

## Load Balancing
Unlike in the previous chapter, where we provisioned and configured our own AWS Application Load Balancer (ALB), when using Amazon Elastic Kubernetes Service (EKS) one of the advantages is that EKS takes on much of the responsability of provisioning and configuring load balancers. We can direct and influence its actions using Kubernetes annotations but this is largely taken care of for us. In our solution, to keep things simple, we'll be using a NGINX as our Ingress Controller and configuring it to setup an AWS Network Load Balancer (NLB) for us.

![Resource][image-3]

_Elastic Load Balancer working with NGINX Ingress Controller to route traffic to our Application's Pods_

In order to delegate this reponsability to EKS we need to grant it the necessary IAM permissions to provision and manage these resources. Therefore, we'll need to again provision an IAM Policy and attach it to the EKS cluster. This is done using an IAM Role that is assigned to the cluster's node group.

![Resource][image-4]

_IAM Policy allowing EKS to provision and manage Elastic Load Balancers_

We then provision Kubernetes resources (e.g., Services and Ingress Controllers) and annotate them to inform the specific configuration of our Elastic Load Balancers that we want EKS to enact on our behalf.

## Network Security

There are many ways to host services on Kubernetes and make them accessible outside of the cluster. In our solution, we'll be using an AWS Elastic Load Balancer to allow external traffic into our cluster through our NGINX Controller. There are other options like NodePort which allow you to access a pod directly through an exposed port on the node. This would require public access to the cluster's nodes and is not the preferred method from both security and scalability perspectives.

If we want access to the cluster using `kubectl` then we need to turn on public endpoint access. This is useful when you are developing something small on your own but not ideal when working in an enterprise context. You will most likely have the private network infrastructure in place to never have to enable the public endpoint.

## Secrets Management
Incorporating secrets into pods within an Amazon Elastic Kubernetes Service (EKS) cluster can be achieved through various methods, each with its own advantages and disadvantages. As we did with virtual machines in the previous chapter, the method that we will explore is using AWS Secrets Manager Secrets. Kubernetes does have its own built-in approach using Kubernetes Secrets. This method is straightforward and integrated directly into Kubernetes, but it has limitations in terms of security, as secrets are encoded in Base64 and can be accessed by anyone with cluster access.

Integration with AWS Secrets Manager can help solve this problem but in order to access our secrets stored in Secrets Manager we need to enable our Kubernetes deployments to authenticate with AWS Identity and Access Management (IAM). This is often referred to as Workload Identity and it is an approach that is relatively common across cloud platforms.

![Resource][image-6]

_AWS Elastic Kubernetes Service (EKS) with Workload Identity_

In order to setup Workload Identity on EKS we need to configure the cluster with an OpenID Connect (OIDC) Provider. Then setup an IAM Role that has a policy that allows a Kubernetes Service Account to assume the role. This IAM Role can then be granted access to any AWS permissions and resources that the Kubernetes deployment needs access to, including Secrets Manager secrets. The last thing we need to do is to provision a Kubernetes Service Account by the same name within Kubernetes and give it a special annotation to connect it to the IAM Role.

Once this is done, our Kubernetes deployments will be allowed to access to our AWS Secrets Manager Secrets but they won't be actually using that access. The final step is to configure the Kubernetes deployment to pull in the secrets and make them accessible to our application code running in the pods.

![Resource][image-7]

_AWS Elastic Kubernetes Service (EKS) Secrets Manager Integration_

Kubernetes has a common practice of doing this using volume mounts. As a result there is a common Kubernetes provider known as the Secrets Store Container Storage Interface (CSI) provider. This is cloud agnostic technique that integrates Kubernetes with external secret stores, such as AWS Secrets Manager. This method offers enhanced security and scalability, but it requires more setup and maintenance. 

In order to get his working we need to deploy two components to our EKS Cluster. First the Secrets Store CSI Driver and then the AWS provider for this driver that will allow it to interface with AWS Secrets Manager. Both of these components we can deploy to our EKS cluster with Helm. Once these important subsystems are in place we can then setup a special Kubernetes resource called a Secret Provider Class. This is a type of resource that connects to AWS Secrets Manager through the CSI driver to access specific secrets. It connects to specific secrets in Secrets Manager using the Service Account that we granted access to them via the IAM Role and its permissions.

## Kubernetes Cluster
Amazon Elastic Kubernetes Service (EKS) offers a managed Kubernetes service that streamlines the deployment and management of containerized applications on AWS. The EKS Cluster is the central figure of this architecture. EKS handles the heavy lifting of setting up, operating, and maintaining the Kubernetes control plane and nodes, which are essentially EC2 instances. When setting up an EKS cluster, users define Node Groups which manifest as collections of EC2 instances that the EKS service is responsible for provisioning and managing.

There are several options for node groups that can host your workloads. The most common examples are AWS- and self-managed node groups. AWS-managed node groups are essentially On-Demand EC2 instances that are allocated for the EKS cluster. AWS simplifies the management of these nodes but that imposes some restrictions on what AWS features can be used. Self-managed nodes are also essentially On-Demand EC2 instances but they provide greater control over the features and configuration options available to them.

A great way to optimize for cost is to use a Farget node group. This option takes advantage of AWS' serverless compute engine and removes the need to provision and manage EC2 instances. However, this is probably more suitable for unpredictable workloads rather than those that require a steady state. In those situations, you can take advantage of a combination of Autoscaling, Spot and Reserved Instances to reap significant discounts and cost reduction.

![Resource][image-5]

_Anatomy of an AWS Elastic Kubernetes Service (EKS) Cluster_

Identity and Access Management (IAM) policies are actually a major part of the configuration of EKS due to the nature of the service and how we delegate responsability to it to manage AWS resoruces. This is similar to what we do with AWS Autoscaling Groups but even more so. IAM Policies are attached to the cluster and to individual node groups. Depending on the capabilities you want to enable within your cluster and your node groups you might need additional policies.

The `AmazonEKSClusterPolicy` policy grants the cluster access to control the internal workings of the cluster itself including Node Groups, CloudWatch logging and access control within the cluster.

The `AmazonEKSVPCResourceController` policy grants the cluster access to management of network resources such as network interfaces, IP address assignment and security groups attachments to the VPC.

There are four policies (e.g., `AmazonEKSWorkerNodePolicy`, `AmazonEKS_CNI_Policy`, `AmazonEC2ContainerRegistryReadOnly`, and `CloudWatchAgentServerPolicy`) that are essential for the operation of EKS worker nodes. These policies absolutely must be attached to the IAM Role that you assign to your EKS Node Group. They grant access to the EKS Cluster's control plane and let nodes within the node group integrate with core infrastructure provided by the cluster including the network, container registries and cloud watch. As described previously, we also add an optional policy to allow the EKS cluster to manage Elastic Load Balancers.


[image-1]:	../images/AWS-VirtualNetwork.png
[image-2]:	../images/AWS-ECR-IAM-Push.png
[image-3]:	../images/AWS-EKS-ALB-NetworkFlow.png
[image-4]:	../images/AWS-EKS-IAM-ALB.png
[image-5]:	../images/AWS-EKS-Cluster.png
[image-6]:	../images/AWS-EKS-WorkloadIdentity.png
[image-7]:	../images/AWS-EKS-IAM-SM.png