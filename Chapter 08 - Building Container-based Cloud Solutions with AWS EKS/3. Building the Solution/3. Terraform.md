# 3. Terraform

As we discussed in our design, our solution is made up of two Application Components: the front- and the back end. Each has its own codebase of application code that needs to be deployed. However, with a Kubernetes solution, the infrastructure is simplified in that we really only need a Kubernetes cluster (and a few other things). The important pieces are the configuration within the Kubernetes platform itself.

As a result, much of the Terraform setup is very similar to what we have done in the previous chapter so we will only focus on new resources needed for our solution. You can check the full source code for this book which is available on GitHub if you want to work with the complete solution.

## Container Registry
First we'll setup repositories for both the front- and back end of our application using AWS Elastic Container Registry (ECR). To simplify the dynamic creation of our ECR repositories we can setup a local variable called `repository_list` that has constants for the two container images we need repositories for.

```
locals {
  repository_list = ["frontend", "backend"]
  repositories    = { for name in local.repository_list : name => name }
}
```

Then we'll use a For-Expression to generate a map from this list that we can then use to create a corresponding ECR repository using the `for_each` iterator.

```
resource "aws_ecr_repository" "main" {

  for_each = local.repositories

  name                 = "ecr-${var.application_name}-${var.environment_name}-${each.key}"
  image_tag_mutability = "MUTABLE"

}
```

Next we'll setup an IAM group that we can grant access to push container images to 

```
resource "aws_iam_group" "ecr_image_pushers" {
  name = "${var.application_name}-${var.environment_name}-ecr-image-pushers"
}
```

Then we'll need to generate an IAM Policy granting access to each of the ECR repositories and attach it to the IAM group we created previously.

```
resource "aws_iam_group_policy" "ecr_image_pushers" {

  for_each = local.repositories

  name  = "${var.application_name}-${var.environment_name}-${each.key}-ecr-image-push-policy"
  group = aws_iam_group.ecr_image_pushers.name

  policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect = "Allow",
        Action = [
          "ecr:GetDownloadUrlForLayer",
          "ecr:BatchGetImage",
          "ecr:BatchCheckLayerAvailability",
          "ecr:PutImage",
          "ecr:InitiateLayerUpload",
          "ecr:UploadLayerPart",
          "ecr:CompleteLayerUpload"
        ],
        Resource = aws_ecr_repository.main[each.key].arn
      }
    ]
  })
}
```

Finally, we can grant access to this group to the identities of developers on our team or the GitHub Actions workflows that will be pushing new images as part of our CI / CD process.

```
resource "aws_iam_group_membership" "ecr_image_pushers" {
  name  = "${var.application_name}-${var.environment_name}-ecr-image-push-membership"
  users = var.ecr_image_pushers
  group = aws_iam_group.ecr_image_pushers.name
}
```

## Kubernetes Cluster

Now that our container registry is all setup and we can push images to it, we need to setup our Kubernetes cluster. That's where AWS Elastic Kubernetes Service (EKS) comes in. The cluster's configuration is relatively simple but there is quite a bit of work we need to do with Identity and Access Management (IAM) to make it all work. 

Before we provision our EKS cluster we need to setup the IAM Role that it will use to interact with the rest of the AWS platform. This is not a role that our nodes or Kubernetes deployments will use. It's the role that EKS will use to enact configuration changes made to the cluster across all the AWS resources that are being used.

```
data "aws_iam_policy_document" "container_cluster_assume_role" {
  statement {
    effect = "Allow"

    principals {
      type        = "Service"
      identifiers = ["eks.amazonaws.com"]
    }

    actions = ["sts:AssumeRole"]
  }
}
```

As a result, the EKS service will assuming this role. Hence, the assume policy needs to allow a principal of type `Service` with `eks.amazonaws.com` as its identifier.

```
resource "aws_iam_role" "container_cluster" {
  name               = "eks-${var.application_name}-${var.environment_name}-cluster-role"
  assume_role_policy = data.aws_iam_policy_document.container_cluster_assume_role.json
}
```

Now, with this role we are going to enable EKS to provision and manage the resources that it needs within our AWS account. As a result, we need to attach the built-in policies `AmazonEKSClusterPolicy` and `AmazonEKSVPCResourceController`.

```
resource "aws_iam_role_policy_attachment" "eks_cluster_policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
  role       = aws_iam_role.container_cluster.name
}
```

The above code is an example of how to do this for one of the policies. You could create a `aws_iam_role_policy_attachment` resource for each of the policies or use an iterator over a collection of the policies that we need to attach. 

Now that this IAM Role is ready we can setup our cluster using the `aws_eks_cluster` resource.

```
resource "aws_eks_cluster" "main" {
  name                      = local.cluster_name
  role_arn                  = aws_iam_role.container_cluster.arn

  vpc_config {

    security_group_ids = [
      aws_security_group.cluster.id,
      aws_security_group.cluster_nodes.id
    ]

    subnet_ids              = local.cluster_subnet_ids
    endpoint_public_access  = true
    endpoint_private_access = true
  }

  // Other configurations like logging, encryption, etc.
}
```

A significant portion of the configuration is done within the `vpc_config` block which references many of the same structures that we have experience provisioning in the previous chapter. 

One thing that you might want to keep in mind is how important the IAM Policies are to enabling this EKS cluster to be successfully provisioned. Since there is no direct relationship between the IAM Role's policy attachments you should ensure that IAM Role permissions are created before we attempt to provision the EKS Cluster. The below code demonstrates the use of the `depends_on` attribute that allows us to define this relationship explicitly.

```
  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_policy,
    aws_iam_role_policy_attachment.eks_vpc_controller_policy,
    aws_cloudwatch_log_group.container_cluster
  ]
```

The EKS Cluster is really just the control plane. In order for our cluster to have utility we need to add worker nodes. This is done by adding one or more Node Groups. These Node Groups will be composed of a collection of EC2 instances that will be enlisted as worker nodes. These nodes also need their own IAM Role.


```
data "aws_iam_policy_document" "container_node_group" {

  statement {
    sid     = "EKSNodeAssumeRole"
    actions = ["sts:AssumeRole"]

    principals {
      type        = "Service"
      identifiers = ["ec2.amazonaws.com"]
    }
  }
}
```

A key differences is that because this role will be assumed by the worker nodes which are EC2 instances, the IAM Role's assume policy needs to align with this fact.

Just as before with our EKS cluster, which needed an IAM Role setup as a pre-requisite, the same is true for our Node Group. Now that the Node Group's IAM Role is ready we can use the code below to create an EKS node group associated with the previously defined cluster. It specifies the desired, minimum, and maximum sizes of the node group, along with other configurations like the AMI type and disk size.

```
resource "aws_eks_node_group" "main" {
  cluster_name    = aws_eks_cluster.main.name
  node_group_name = "ng-user"
  node_role_arn   = aws_iam_role.container_node_group.arn
  subnet_ids      = local.cluster_subnet_ids

  scaling_config {
    desired_size = 3
    min_size     = 1
    max_size     = 4
  }

  ami_type       = var.node_image_type
  instance_types = [var.node_size]

}
```

Again, just like with the EKS Cluster, the IAM Role's policy attachments are critical to making the Node Group functional. Therefore, you need to make sure that all policy attachments are attached to the IAM Role before we start provisioning our Node Group. As we discussed in the previous section, there are four policies (e.g., `AmazonEKSWorkerNodePolicy`, `AmazonEKS_CNI_Policy`, `AmazonEC2ContainerRegistryReadOnly`, and `CloudWatchAgentServerPolicy`) that are essential for the operation of EKS worker nodes. 

```
  depends_on = [
    aws_iam_role_policy_attachment.eks_worker_node_policy,
    aws_iam_role_policy_attachment.eks_cni_policy,
    aws_iam_role_policy_attachment.eks_ecr_policy,
	aws_iam_role_policy_attachment.eks_cloudwatch_policy
  ]
```

As you add additional features to your EKS cluster you may introduce additional IAM policies that grant the cluster and its worker nodes different permissions within AWS. When you do, don't forget to also include these policies into these `depends_on` attributes to ensure smooth operations.

## Logging and Monitoring

We can enable CloudWatch logging on the cluster very easily by simply adding the `enabled_cluster_log_types` attribute to the `aws_eks_cluster` resource.

```
enabled_cluster_log_types = ["api", "audit"]
```

This attribute takes one or more different log types. I'd recommend checking the documentation for all the different options supported. Next we need to provision a CloudWatch Log Group for the cluster.

```
resource "aws_cloudwatch_log_group" "container_cluster" {
  name              = "/aws/eks/${local.cluster_name}/cluster"
  retention_in_days = 7
}
```

This requires a specific naming convention and it needs to match exactly with the name you use for your cluster. Therefore, it's a good idea to extract the value you pass to the `name` attribute of the `aws_eks_cluster` resource as a local variable so you can use it in two places.

## Workload Identity

With the cluster provisioned we need to get the OIDC issuer certificate from the cluster to use it to configure the OpenID Connect provider with AWS Identity and Access Management. The below code uses the `tls_certificate` data source from the `tls` utility provider which we covered in Chapter 3 to obtain additional metadata about the certificate.

```
data "tls_certificate" "container_cluster_oidc" {
  url = aws_eks_cluster.main.identity[0].oidc[0].issuer
}
```

With this additional metadata we can then use the `aws_iam_openid_connect_provider` resource to connect the cluster to the AWS IAM OIDC Provider by referencing `sts.amazonaws.com`. 

```
resource "aws_iam_openid_connect_provider" "container_cluster_oidc" {
  client_id_list  = ["sts.amazonaws.com"]
  thumbprint_list = [data.tls_certificate.container_cluster_oidc.certificates[0].sha1_fingerprint]
  url             = data.tls_certificate.container_cluster_oidc.url
}
```

We've already setup several IAM Roles, including one for the EKS cluster and another for the worker nodes of the cluster. Therefore, I won't reiterate the creation of the `aws_iam_role` resource for the workload identity. However, this new role does need to have a very distinct assumption policy. The Workload Identity IAM Role needs to reference the OIDC Provider and a yet-to-be-provisioned Kubernetes Service Account.

```
data "aws_iam_policy_document" "workload_identity_assume_role_policy" {
  statement {
    actions = ["sts:AssumeRoleWithWebIdentity"]
    effect  = "Allow"

    condition {
      test     = "StringEquals"
      variable = "${replace(aws_iam_openid_connect_provider.container_cluster_oidc.url, "https://", "")}:sub"
      values   = ["system:serviceaccount:${var.k8s_namespace}:${var.k8s_service_account_name}"]
    }

    principals {
      identifiers = [aws_iam_openid_connect_provider.container_cluster_oidc.arn]
      type        = "Federated"
    }
  }
}
```

As you can see, in the above code the Service Account follows a very specific naming convention: `system:serviceaccount:<namespace>:<service-account-name>`. The `<namespace>` we replace with the name of the Kubernetes namespace and likewise, the `<service-account-name>` we replace with the name of the service account. It's important to call out that we are referencing resources that do not exist yet. As such, the reference to them within the Workload Identity IAM Role's assumption policy is a pointer or a placeholder to this yet-to-be created resource. Both the Kubernetes namespace and the service account are resources that will need to be created within the Kubernetes control plane. We'll tackle that in the next section using the `kubernetes` Terraform provider.

## Secrets Management

Now that we have an IAM Role for our Workload Identity we simply need to grant it access to the AWS resources we want it to use. Therefore, we will again use the `aws_iam_policy_document` data source to generate an IAM policy that we will attach to the Workload Identity's IAM Role. This is where we have the opportunity to grant it access to any resource in AWS that our application code will need. For our solution, we'll start with access to AWS Secrets Manager secrets by granting access to read secrets using the `secretsmanager:GetSecretValue` action.

```
data "aws_iam_policy_document" "workload_identity_policy" {
  statement {
    effect = "Allow"

    actions = [
      "secretsmanager:GetSecretValue",
      "secretsmanager:DescribeSecret",
    ]

    resources = [
      "arn:aws:secretsmanager:${var.primary_region}:${data.aws_caller_identity.current.account_id}:secret:*",
    ]
  }
}
```

This policy will grant the IAM Role access to the secrets within this account. We could further refine its access by enhancing the `*` wildcard path to ensure that it has access to only certain secrets. This can be done by implementing a naming convention that uses a unique prefix for your secrets. The `application_name` and `environment_name` variables are a perfect way to implement this naming convention and to tighten access to your Kubernetes workloads to AWS Secrets Manager.

Now we just need to provision secrets to Secrets Manager with the right naming convention.
```
resource "aws_secretsmanager_secret" "database_connection_string" {
  name        = "${var.application_name}-${var.environment_name}-connection-string"
  description = "Database connection string"
}
```

AWS Secrets Manager uses a parent resource `aws_secretsmanager_secret` as a logical placeholder for the secret itelf but recognizes that the secret's value might change over time.

```
resource "aws_secretsmanager_secret_version" "database_connection_string" {
  secret_id     = aws_secretsmanager_secret.database_connection_string.id
  secret_string = random_password.database_connection_string.result
}
```

Those different values for the secret are stored in `aws_secretsmanager_secret_version` resources. You can generate your own complex secrets usign the `random` provider but its probably more common to obtain the `secret_string` from the outputs of other resources.