# 4. Kubernetes

In Chapter 5 we first introduced Kubernetes architecture and automation techniques using YAML and HashiCorp Configuration Language. In our solutions in this book we will be using the Terraform provider for Kubernetes to automate our application's deployment. This allows us to both parameterize the Kubernetes configurations that would otherwise be trapped in hard-coded YAML files and provision a combination of Kubernetes primitives and Helm Charts with the same deployment process.

## Provider Setup

Ironically, the first thing we need to do in order to setup the `kubernetes` provider is to initialize the `aws` provider so we can get information about our EKS cluster. We can do that using data sources provided and a single input variable: the cluster's name. Of course, the AWS region is also an implied parameter to this operation but it is part of the `aws` provider configuration rather than inputs to the data sources themselves.

```
data "aws_eks_cluster" "cluster" {
  name = var.eks_cluster_name
}
```

We'll use both the `aws_eks_cluster` and the `aws_eks_cluster_auth` data sources in order to grab the data we need to initialize the `kubernetes` provider.

```
provider "kubernetes" {
  host                   = data.aws_eks_cluster.cluster.endpoint
  cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority[0].data)
  token                  = data.aws_eks_cluster_auth.cluster.token
  load_config_file       = false
}
```

Interestingly, the Helm provider setup is pretty much identical to the Kubernetes provider configuration. It seems a bit redundant but its relatively straight forward.

```
provider "helm" {
  kubernetes {
    host                   = data.aws_eks_cluster.main.endpoint
    cluster_ca_certificate = base64decode(data.aws_eks_cluster.main.certificate_authority[0].data)
    token                  = data.aws_eks_cluster_auth.main.token
  }
}
```

## Namespace

Creating the Kubernetes namespace is extremely simple. 

```
resource "kubernetes_namespace" "main" {
  metadata {
    name = var.k8s_namespace
    labels = {
      name = var.k8s_namespace
    }
  }
}
```

This will act as the logical container for all of the Kubernetes resources that we provision for our application.

## Service Account

In the previous section we built one half of this bridge when we setup the OpenID Connect provider configuration within AWS and we specified the Kubernetes namespace and service account name ahead of time. Now we finish construction of this bridge by provisioning the `kubernetes_service_account` and ensuring the `namespace` and `name` match our AWS configuration.

```
resource "kubernetes_service_account" "workload_identity" {
  metadata {
    name      = var.k8s_service_account_name
    namespace = var.k8s_namespace
    annotations = {
      "eks.amazonaws.com/role-arn" = var.workload_identity_role
    }
  }
}
```

We also need to add an annotation that references the unique identifier (or ARN) for the Workload Identity's IAM Role. This we can setup as an output variable in our Terraform workspace that provisions the AWS infrastructure and route it's value to an input variable on the Terraform workspace for our Kubernetes configuration. This is a great example of how the `kubernetes` provider for Terraform can be a useful way of configuring Kubernetes resources that require tight coupling with the cloud platform.

## Secrets Store CSI Driver

With the service account setup our application is one step closer to being able to access our secrets in AWS Secrets Manager. However, before we can do that we need to setup the Secrets Store CSI Driver. As we discussed previously, this is a common Kubernetes component that provides a standard mechanism for using volume mounts as a way to distribute remotely managed secrets to workloads running in Kubernetes. The driver is extremely flexible and can be extended through providers that act as adapters for different external secret management systems.

First we need to install the Secrets Store CSI Driver Helm chart. 

```
resource "helm_release" "csi_secrets_store" {

  name       = "csi-secrets-store"
  repository = "https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts"
  chart      = "secrets-store-csi-driver"
  namespace  = "kube-system"

  set {
    name  = "syncSecret.enabled"
    value = "true"
  }

}
```

We can optionally enable secret synchronization, using the `syncSecret.enabled` attribute, to make the secrets accessible from Kubernetes secrets. This makes it extremely convenient to inject the secrets into our application's pods without customized code to retrieve them from the mounted volume.

Next we need to install the AWS provider for the CSI Driver.

```
resource "helm_release" "aws_secrets_provider" {

  name       = "secrets-provider-aws"
  repository = "https://aws.github.io/secrets-store-csi-driver-provider-aws"
  chart      = "secrets-store-csi-driver-provider-aws"
  namespace  = "kube-system"

}
```

Both of these Helm charts provision a number of different Kubernetes resources to your cluster under the `kube-system` namespace. If you encounter errors, interrogating the pods hosting this components is a good place to start to debug your configuration.

## Secret Provider Class

Once we have installed both the CSI Driver and it's AWS provider we are ready to connect to AWS Secrets Manager. Up to this point, we have only enabled this ability we haven't actually exercised it by accessing secrets.

That's what the Secret Provider Class resource is for. It connects to a specific set of secrets within AWS Secrets Manager. You'll notice that the way this type of resource is provisioned is different than other resources in Kubernetes. While other resource types have a corresponding Terraform resource, the Secret Provider Class uses a `kubernetes_manifest` resource. 

That is because this resource type is managed through a Kubernetes Custom Resource Definition (CRD), it's not a built-in type within Kubernetes.

```
resource "kubernetes_manifest" "secret_provider_class" {

  manifest = {
    apiVersion = "secrets-store.csi.x-k8s.io/v1"
    kind       = "SecretProviderClass"
    metadata = {
      name      = "${var.application_name}-${var.environment_name}-secret-provider-class"
      namespace = var.k8s_namespace
    }
    spec = {
      provider = "aws"
      parameters = {
        objects = yamlencode([ ... ])
      }
      secretObjects = [ ... ]
    }
  }

}
```

The structure of the Secret Provider Class has two parts. First, the `parameters` is where we declare what secrets we want to bring in. 

```
{
  objectName         = "fleet-portal-dev-connection-string"
  objectType         = "secretsmanager"
  objectVersionLabel = "AWSCURRENT"
}
```

The `objectName` corresponds to either the relative name of the Secrets Manager secret or a fully qualified ARN for the secret. The `objectType` indicates what CSI Driver provider should be used to access the secret and the `objectVersionLabel` allows us to select a specific version of the secret within Secrets Manager. For AWS, to access the latest version (probably the most common use case) you need to specify `AWSCURRENT` as the value.

Next, there is a collection of `secretObjects` which is used to define corresponding Kubernetes Secret Objects.

```
{
  data = [
    {
      key        = "fleet-portal-dev-connection-string"
      objectName = "fleet-portal-dev-connection-string"
    }
  ]
  secretName = "fleet-portal-dev-connection-string"
  type       = "Opaque"
}
```

These `secretObjects` will later be used in the deployment specification of our application to create environment variables for each secret within the pods.

## Deployment

The Kubernetes deployment is one of the most significant resources that we have to provision within Kubernetes. As a result, it can be rather intimidating as there are several rather complex nested sections. The most important thing going on in the deployment is the container specification. This sets up the actual runtime environment for our pods.

Probably the most important piece of information is the container image we want to use in our pods. In order to configure this we need to construct the fully qualified path to the container image stored in our ECR. In order to do that we need two pieces of information. First we need the AWS Account Number and second, we need the AWS Region name where our ECR repository is provisioned to.

```
locals {
  account_id         = data.aws_caller_identity.current.account_id
  container_registry = "${local.account_id}.dkr.ecr.${var.primary_region}.amazonaws.com/"
}
```

The AWS Account Number can be easily obtained from the `aws_caller_identity` data source. This is an extremely simple data source that provides contextual information about the AWS Account and IAM Identity that Terraform is using with the `aws` provider. As a result, in order to create this data source you simply create it without any parameters.

```
data "aws_caller_identity" "current" {}
```

This is a common pattern for accessing Terraform provider authentication context and cloud platform provisioning scope, in this case what AWS Account and what region we are provisioning to.

Here is the version of the same YAML code converted to HCL and using input variable to set different attributes on the entity.

```
resource "kubernetes_deployment" "web_app" {
  metadata {
    name      = local.web_app_name
    namespace = var.k8s_namespace
  }

  spec {
    replicas = 3

    selector {
      match_labels = {
        app = local.web_app_name
      }
    }

    template {
      metadata {
        labels = {
          app = local.web_app_name
        }
      }

      spec {
        service_account_name = kubernetes_service_account.workload_identity.metadata[0].name

        container {
          image = local.web_app_image_name
          name  = local.web_app_name
          port {
            container_port = 5000
          }
          env_from {
            config_map_ref {
              name = kubernetes_config_map.web_app.metadata.0.name
            }
          }
        }
      }
    }
  }

}
```

The local variable we use for the container image name is the fully qualified path to our container image within the ECR. It follows the following structure: `<account>.dkr.ecr.<region>.amazonaws.com/<repository>:<tag>`. The `<account>` is the AWS Account Number which is accessible using the `aws_caller_identity` data source. The `<region>` is the AWS region which is accessible from the input variables. The `<repository>` is the ECR repository name and the `<version>` is the tag for the specific version of the container image. 

We can set the `service_account_name` by referencing other Kubernetes resources provisioned within this Terraform workspace. This is a key difference between using YAML and the `kubernetes` provider for Terraform. If we were using YAML this would have to be hard coded, whereas with HashiCorp Configuration Language we can reference other resources within the Terraform workspace.

In order to reference an AWS Secrets Manager secret we would need to modified the `container` block to include another `env` block.

```
env {
  name = "DB_CONNECTION_STRING"
  value_from {
    secret_key_ref {
      name = "fleet-portal-dev-connection-string"
      key  = "fleet-portal-dev-connection-string"
    }
  }
}
```

This allows us to reference one of the `secretObjects` we declared within the Secret Provider Class and to give it an environment variable name that our application code can reference to access the secret.

## Service

The Kubernetes service is primarily a network routing mechanism. It defines the port on which the service should be exposed to external clients and what port the network traffic should be forwarded to on the pods.

```
resource "kubernetes_service" "web_app" {
  metadata {
    name      = "${local.web_app_name}-service"
    namespace = var.k8s_namespace

  }
  spec {
    type = "ClusterIP"
    port {
      port        = 80
      target_port = 5000
    }
    selector = {
      app = local.web_app_name
    }
  }
}
```

The `selector` specifies which pods traffic should be forwarded to and it should match the corresponding pods with the label `app` set to the same value as the service's selector.

## ConfigMap

As we know from Chapter 5, the ConfigMap resource is a great way to pass non-sensitive configuration settings to your pods.

```
resource "kubernetes_config_map" "web_app" {
  metadata {
    name      = "${local.web_app_name}-config"
    namespace = var.k8s_namespace
  }

  data = {
    BackendEndpoint = ""
  }
}
```

Oftentimes, the Terraform workspace that provisions the infrastructure will output a number of different values that need to be included in a Kubernetes ConfigMap (URIs, AWS ARNs, DNS, etc.).

## Ingress

The Ingress Controller is a component of Kubernetes that manages the routing of external network traffic into the cluster. It works in conjunction with a Kubernetes ingress that defines specific rules that route traffic for specific services. This is very similar to the structure of the CSI Driver and the Secret Provider Class. One provides the foundational sub-system, thus enabling the capability, while another implements a specific configuration usign that underlying sub-system.

One of the most popular ingress controllers is a load balancer called NGINX. We can setup NGINX ingress controller using a Helm Chart. The components deployed by this Helm Chart are why we needed an additional IAM policy that allows our EKS cluster to configure AWS Elastic Load Balancing Resources. That's because the Kubernetes configurations of the ingress controller and ingress resources will be interpretted by EKS and manifested as the provisioning and configuration of AWS Elastic Load Balancing Resources. That means instead of explicitly configuring ELB resources using the `aws` Terraform provider you will be annotating Kubernetes deployments and the necessary ELB resources will be provisioned and configured on your behalf.

The first thing we need to do is install the NGINX ingress controller using a Helm Chart.

```
resource "helm_release" "ingress" {
  name       = "ingress"
  repository = "https://charts.bitnami.com/bitnami"
  chart      = "nginx-ingress-controller"

  create_namespace = true
  namespace        = "ingress-nginx"

  set {
    name  = "service.type"
    value = "LoadBalancer"
  }
  set {
    name  = "service.annotations"
    value = "service.beta.kubernetes.io/aws-load-balancer-type: nlb"
  }

}
```

This will install NGINX and deploy a Kubernetes service for NGINX running under the namespace we specified. The next step is to configure an ingress for our application.

```
resource "kubernetes_ingress_v1" "ingress" {
  metadata {
    name      = "${local.web_app_name}-ingress"
    namespace = var.k8s_namespace
    annotations = {
      "kubernetes.io/ingress.class" = "nginx"
    }
  }
  spec {
    rule {
      http { ... }
    }
  }
}
```

An ingress resource is pretty simple. You need to set the namespace and specify what ingress controller you want to use. Then you need to specify paths to route network traffic to the correct Kubernetes services.

```
path {
  path      = "/"
  path_type = "Prefix"

  backend {
    service {
      name = kubernetes_service.web_app.metadata[0].name
      port {
        number = 80
      }
    }
  }
} 
```

It's also pretty important to establish explicit `depends_on` statements for the Kubernetes services for the front- and back end application deployments as well as the ingress controller since we don't reference it directly within the HashiCorp Configuration Language configuration.

```
depends_on = [
  kubernetes_service.web_app,
  kubernetes_service.web_api,
  helm_release.ingress
]
```