# 1. Overview

Kubernetes is a platform that expands on the responsibilities of the Container runtime, which operates at an individual host level. Kubernetes’ job is to perform this across multiple nodes. As we learned in the first section of this chapter, the Container runtime uses a Linux Operating System construct—control groups—to protect the health of the Operating System by ensuring that the physical (or virtual) host that the containers are running on remains healthy. Kubernetes essentially does the same thing but across many, many servers.

Most applications or systems will naturally be organized into different components, layers, or micro-services—each with its own responsibilities and corresponding application code and technology stack that implements its functionality. Each component within such a system will have its own container that has this software installed.

When we deploy systems using Virtual Machines, we do so in such a way that the same component is deployed to two more Virtual Machines, and we ensure that these Virtual Machines do not share the same underlying physical equipment. This separation could be as simple as a different physical host in the same rack all the way up to a different physical host in an entirely different data center—sometimes separated by many tens—if not hundreds—of miles. This allows us to achieve high availability and resiliency during an outage or an issue affecting some underlying component of the physical hardware.

Unlike when using VMs, our application components don’t sit on isolated VMs; they sit on the cluster nodes, oftentimes with pods from other applications.

Kubernetes tries to make sure that our application containers don’t sit on the same node. That way, if one of the cluster’s nodes fails, our application will not go down. Kubernetes also takes it a step further by intelligently reorganizing the containers on other health nodes. In order to do this, Kubernetes maintains a divide between its own internal “Logical Layer” and the underlying “Physical Layer” and maps the device by assigning Logical deployments, or Pods, to physical deployments and nodes. This separation between the logical and the physical is one of Kubernetes’ huge advantages, which makes it so effective at managing applications and services on top of a potentially unlimited underlying physical infrastructure.

![Logical-Physical Divide][image-1]

That’s pretty much it but there are a lot of ways we can customize how our application’s components are deployed to Kubernetes to meet the specific needs of our application.

Kubernetes is flexible enough to run on a fleet of Virtual Machines on a cloud provider or physical bare metal servers down to running on a single computer—like your laptop. This flexibility makes it an ideal choice for hybrid cloud scenarios. It streamlines the problematic task of integration testing by allowing developers to run a copy of the entire solution on their laptop that closely mimics a production environment.

Kubernetes offers a rich set of features that fulfill most of the needs for running workloads at scale, such as service discovery, secrets management, horizontal scaling, automated rollouts and rollbacks, and self-healing capabilities—making it an ideal candidate to run both stateless and stateful applications at scale while avoiding vendor lock-in.

Kubernetes architecture is a set of loosely coupled and extensible components. This modularity allows adaptations for different cloud providers to integrate with their specific solutions for networking, storage, service mesh, etc.

Like Terraform, Google designed Kubernetes to encourage the adoption of Infrastructure-as-Code by leveraging a declarative approach for defining your application’s runtime environment. Due to the extensibility of both Terraform and Kubernetes, several integration options exist. In this chapter, we’ll discuss a few of those approaches and the trade-offs that come along with each—but before we do that, we need to introduce some critical concepts of Kubernetes internal architecture and operating model. Only with this foundation can we maximize the potential of leveraging Terraform and Kubernetes together.


[image-1]:	../images/Kubernetes-Pods-Nodes.png