# 3. Terraform

As we discussed in our design, our solution is made up of two Application Components: the Frontend and the Backend. Each has its own application codebase that needs to be deployed. Since this is the first time we will be using the `azurerm` provider, we’ll look at the basic provider setup and the configuration of the backend before we move into the nuts and bolts of each component of our architecture.

## Provider Setup

We need to specify all the providers that we intend to use in this solution within the `required_providers` block.

	terraform {
	  required_providers {
	    azurerm = {
	      source  = "hashicorp/azurerm"
	      version = "~> 3.75.0"
	    }
	    cloudinit = {
	      source  = "hashicorp/cloudinit"
	      version = "~> 2.3.2"
	    }
	  }
	  backend "azurerm" {
	  }
	}

We’ll also configure the Azure provider. Unlike the AWS provider, the Azure provider is not scoped to a particular region. This means you can provision resources across all Azure regions without declaring different Azure provider blocks.

	provider "azurerm" {
	  features {}
	}

The Azure provider does require some additional parameters to specify the credentials to use to connect to Azure, but because these are sensitive values, we don’t want to embed them into the code. We’ll pass those values in later when we automate the deployment using the standard Azure credentials environment variables:

- **Tenant ID:** `ARM_TENANT_ID`
- **Subscription ID:** `ARM_SUBSCRIPTION_ID`
- **Client ID:** `ARM_CLIENT_ID`
- **Client Secret:** `ARM_CLIENT_SECRET`

## Backend

Because we will be using a CI/CD pipeline to provision and maintain our environment long term we need to setup a remote backend for our Terraform State. Because our solution will be hosted on Azure, we’ll use the Azure Blob Storage backend to store our Terraform State.

Just like the Azure provider, we don’t want to hard code the backend configuration in our code so we simply setup a placeholder for the backend.

	terraform {
	
	  ...
	
	  backend "azurerm" {
	  }
	}

We’ll configure the backend’s parameters using the `-backend-config` parameters when we run `terraform init` in our CI/CD pipeline.

## Input Variables

It’s good practice to pass in short names that identify the application’s name and the application’s environment. This allows you to embed consistent naming conventions across the resources that make up your solution which makes it easier to identify and track resources from the Azure Portal.

The `primary_region`, `vnet_cidr_block` and `az_count` input variables drive key architectural characteristics of the deployment. It is important that they are not hard-coded as it would limit the reusability of the Terraform codebase. 

The `vnet_cidr_block` establishes the Virtual Network address space, which is often tightly regulated by an enterprise governance body. There is usually a process to ensure that teams across an organization do not use IP address ranges that conflict, thus making it impossible in the future to allow those two applications to integrate with one another or integrate with shared network resources within the enterprise.

The `az_count` allows for configurability of how much redundancy we want within our solution. This will affect the high availability of the solution but also the cost of the deployment. As you can imagine, cost, is also a tightly regulated characteristic of cloud infrastructure deployments.

## Consistent Naming & Tagging

Unlike the AWS Console, Azure is designed in such a way that it is extremely easy to get an application-centric view of your deployment: Resource Groups!

	resource "aws_vpc" "main" {
	  cidr_block = var.vpc_cidr_block
	
	  tags = {
	    Name        = "${var.application_name}-${var.environment_name}-network"
	    application = var.application_name
	    environment = var.environment_name
	  }
	}
	resource "azurerm_virtual_network" "main" {
	
	  ...
	
	  tags = {
	    application = var.application_name
	    environment = var.environment_name
	  }
	
	}

It’s still important to tag the resources that you deploy that indicate what application and what environment they belong to. This helps for other reporting needs like budgets and compliance. Almost all resources within the Azure provider have a `map` attribute called `tags`. Unlike AWS, each resource has a `name` as a required attribute. 

## Virtual Network

Just as we did in Chapter 7, we need to construct a Virtual Network and keep its address space as tight as possible to avoid gobbling up unnecessary address space for the broader organization in the future.

	resource "azurerm_virtual_network" "main" {
	
	  name                = "vnet-${var.application_name}-${var.environment_name}"
	  location            = azurerm_resource_group.main.location
	  resource_group_name = azurerm_resource_group.main.name
	  address_space       = [var.vnet_cidr_block]
	
	}

The network creation in Azure is simpler than what we did with AWS because we don’t have to segment our subnets based on Availability Zone.

	resource "azurerm_subnet" "frontend" {
	
	  name                 = "snet-frontend"
	  resource_group_name  = azurerm_resource_group.main.name
	  virtual_network_name = azurerm_virtual_network.main.name
	  address_prefixes     = [cidrsubnet(var.vnet_cidr_block, 2, 1)]
	
	}

## Load Balancing

As we discussed in the design, the Azure Load Balancer service is structured quite a bit differently than AWS’s equivalent offering. 

	
	resource "azurerm_public_ip" "frontend" {
	  name                = "pip-lb-${var.application_name}-${var.environment_name}-frontend"
	  location            = azurerm_resource_group.main.location
	  resource_group_name = azurerm_resource_group.main.name
	  allocation_method   = "Static"
	  sku                 = "Standard"
	  zones               = [1, 2, 3]
	}
	
	resource "azurerm_lb" "frontend" {
	  name                = "lb-${var.application_name}-${var.environment_name}-frontend"
	  location            = azurerm_resource_group.main.location
	  resource_group_name = azurerm_resource_group.main.name
	  sku                 = "Standard"
	
	  frontend_ip_configuration {
	    name                 = "PublicIPAddress"
	    public_ip_address_id = azurerm_public_ip.frontend.id
	    zones                = [1, 2, 3]
	  }
	}

It’s important to call out that in order to achieve Zonal Resiliency we need to ensure that all components of our architecture are deployed in a Zone Resilient way. This often requires setting the `zones` attribute and specifying which Availability Zones we want to provision into.

The backend configuration of the Azure Load Balancer is a simple logical container for the Backend Address Pool.

	resource "azurerm_lb_backend_address_pool" "frontend" {
	  loadbalancer_id = azurerm_lb.frontend.id
	  name            = "frontend-pool"
	}

This logical container must be linked to either static Virtual Machines or a Virtual Machine Scale Set.

	resource "azurerm_network_interface_backend_address_pool_association" "frontend" {
	
	  count = var.az_count
	
	  network_interface_id    = azurerm_network_interface.frontend[count.index].id
	  ip_configuration_name   = "internal"
	  backend_address_pool_id = azurerm_lb_backend_address_pool.frontend.id
	
	}

Notice how in the above Backend Address Pool Association resource we are iterating over the `var.az_count`. This is the same number that we iterate over the Virtual Machines, which allows us to put a single Virtual Machine into each Availability Zone.\_ 
Unlike AWS, where the Load Balancer Rules are split between a Listener and a Target Group configuration, an Azure Load Balancer Rule combines the two and then links them to a corresponding Health Probe.

	resource "azurerm_lb_probe" "frontend_probe_http" {
	  loadbalancer_id = azurerm_lb.frontend.id
	  name            = "http"
	  protocol        = "Http"
	  port            = 5000
	  request_path    = "/"
	}
	
	resource "azurerm_lb_rule" "frontend_http" {
	  loadbalancer_id                = azurerm_lb.frontend.id
	  name                           = "HTTP"
	  protocol                       = "Tcp"
	  frontend_port                  = 80
	  backend_port                   = 5000
	  frontend_ip_configuration_name = "PublicIPAddress"
	  probe_id                       = azurerm_lb_probe.frontend_probe_http.id
	  backend_address_pool_ids       = [azurerm_lb_backend_address_pool.frontend.id]
	  disable_outbound_snat          = true
	}

Notice how the Load Balancer Rule is connecting many of the components together including the Front End IP Configuration, the Listener on AWS, the Health Probe and the Backend Address Pool—the Target Group on AWS.

## Network Security

First we need to setup the logical Application Security Group for each Application Architectural component. We’ll have one for the Frontend and one for the Backend.

	resource "azurerm_application_security_group" "frontend" {
	
	  name                = "asg-${var.application_name}-${var.environment_name}-frontend"
	  resource_group_name = azurerm_resource_group.main.name
	  location            = azurerm_resource_group.main.location
	
	}

Next we need to create Network Security Groups (NSG) that allow the necessary traffic in to each of the Application Security Groups.

	resource "azurerm_network_security_group" "frontend" {
	
	  name                = "nsg-${var.application_name}-${var.environment_name}-frontend"
	  resource_group_name = azurerm_resource_group.main.name
	  location            = azurerm_resource_group.main.location
	
	}
	
	resource "azurerm_network_security_rule" "frontend_http" {
	
	  resource_group_name                        = azurerm_resource_group.main.name
	  network_security_group_name                = azurerm_network_security_group.frontend.name
	  name                                       = "allow-http"
	  priority                                   = "2001"
	  access                                     = "Allow"
	  direction                                  = "Inbound"
	  protocol                                   = "Tcp"
	  source_port_range                          = "*"
	  destination_port_range                     = "5000"
	  source_address_prefix                      = "*"
	  destination_address_prefix                 = "*"
	  destination_application_security_group_ids = [azurerm_application_security_group.frontend.id]
	
	}


## Secrets Management

First, we’ll set up a Key Vault.

	resource "azurerm_key_vault" "main" {
	  name                       = "kv-${var.application_name}-${var.environment_name}"
	  location                   = azurerm_resource_group.main.location
	  resource_group_name        = azurerm_resource_group.main.name
	  tenant_id                  = data.azurerm_client_config.current.tenant_id
	  soft_delete_retention_days = 7
	  purge_protection_enabled   = false
	  sku_name                   = "standard"
	  enable_rbac_authorization  = true
	}

Then, we’ll set up a Managed Identity for each Application Architectural component.

	resource "azurerm_user_assigned_identity" "frontend" {
	
	  name                = "${var.application_name}-${var.environment_name}-frontend"
	  location            = azurerm_resource_group.main.location
	  resource_group_name = azurerm_resource_group.main.name
	
	}

Then, we’ll grant the Managed Identity the necessary privileges using Azure Role Assignments.

	resource "azurerm_role_assignment" "frontend_keyvault" {
	  scope                = azurerm_key_vault.main.id
	  role_definition_name = "Key Vault Secrets User"
	  principal_id         = azurerm_user_assigned_identity.frontend.principal_id
	}

## Virtual Machines

First, we’ll obtain the Virtual Machine Image from our input variables. We built This Virtual Machine Image with Packer and provisioned into a different Azure Resource Group.

	data "azurerm_image" "frontend" {
	  name                = var.frontend_image.name
	  resource_group_name = var.frontend_image.resource_group_name
	}

Then, we’ll create the Network Interface for each Virtual Machine by iterating over the `var.az_count` input variable.

	resource "azurerm_network_interface" "frontend" {
	
	  count = var.az_count
	
	  name                = "nic-${var.application_name}-${var.environment_name}-frontend${count.index}"
	  location            = azurerm_resource_group.main.location
	  resource_group_name = azurerm_resource_group.main.name
	
	  ip_configuration {
	    name                          = "internal"
	    subnet_id                     = azurerm_subnet.frontend.id
	    private_ip_address_allocation = "Dynamic"
	  }
	}

Finally, we’ll set up the Virtual Machine with all the necessary attributes and link it to the Network Interface, the Virtual Machine Image, and the Managed Identity.

	resource "azurerm_linux_virtual_machine" "frontend" {
	
	  count = var.az_count
	
	  name                = "vm-${var.application_name}-${var.environment_name}-frontend${count.index}"
	  resource_group_name = azurerm_resource_group.main.name
	  location            = azurerm_resource_group.main.location
	  size                = "Standard_F2"
	  admin_username      = var.admin_username
	  zone                = count.index + 1
	
	  network_interface_ids = [
	    azurerm_network_interface.frontend[count.index].id
	  ]
	
	  admin_ssh_key {
	    username   = var.admin_username
	    public_key = tls_private_key.ssh.public_key_openssh
	  }
	
	  os_disk {
	    caching              = "ReadWrite"
	    storage_account_type = "Standard_LRS"
	  }
	
	  source_image_id = data.azurerm_image.frontend.id
	  user_data       = data.cloudinit_config.frontend.rendered
	
	}
