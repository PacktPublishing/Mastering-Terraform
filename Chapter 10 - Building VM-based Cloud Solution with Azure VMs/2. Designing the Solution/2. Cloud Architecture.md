# 2. Cloud Architecture

In Chapter 7, we developed a similar solution using Amazon Web Services (AWS) and its equivalent offerings with regard to Virtual Machines. As a result, our design for Azure will look rather similar. Many of the cloud services we use on AWS have equivalents to Microsoft Azure. This is largely due to the fact that virtual machines, networks, and network security have stabilized in terms of how the industry views them. Don’t expect to see radical differences in naming conventions and how things work. When working with this cloud computing paradigm, the differences between platforms are usually very subtle. Throughout the book, I will attempt to highlight synonymous terms across clouds to help you better translate your conceptual knowledge from one cloud to another.

## Virtual Network
Virtual Machines must be deployed within a Virtual Network. If you recall from Chapter 7 when we provisioned this solution on AWS we needed to setup multiple Subnets in order for our solution to span Availability Zones. That is because of the structure of virtual networks on AWS, how the virtual network is scoped to an AWS region, and how a subnet is scoped to an AWS availability zone. Azure is different.

On Azure, the Virtual Network and the Subnets are scoped to a Region. Zonal Resiliency is built into the Virtual Network. In fact, Azure has two resiliency modes: one based on Fault Domains or “Regional” and another based on Availability Zones or “Zonal”. Virtual Machines can be provisioned in either of these two modes: Regional or Zonal. 

In order to provision a Regional Virtual Machine solution, you need to provision an Availability Set and specify how many Fault Domains you want to distribute your Virtual Machines across. When Virtual Machines are provisioned within this Availability Set, the Azure platform takes care to ensure that they are provisioned to hardware that does not share a common source of power and network switch—thus making it less likely that the entire workload will fail in the case of an outage isolated to a single Fault Domain. If you don’t use an availability set, Azure will allocate your virtual machines based on available capacity and make no guarantee that your virtual machines won’t be in the same fault domain.

In order to provision a Zonal Virtual Machine solution, you need to simply specify which Availability Zone to use to provision your Virtual Machine and ensure that you have more than one Virtual Machine spread across multiple Availability Zones. An availability zone offers much more resiliency than a fault domain, as instead of the Azure platform guaranteeing your virtual machine does not share the same power source and network switch, it guarantees your virtual machine is in a totally different physical data center within the region. In this book, we will focus on ensuring that our solution achieves Zonal Resiliency.

![Resource][image-1]
_Azure Virtual Network Architecture_

In the above diagram, you can see that our Virtual Network and both its subnets can support Virtual Machines across all Availability Zones within the Region.

![Resource][image-2]
_Isolated Subnets for Frontend and Backend Application Components_

This means that we don’t need to design our Subnets based on the constraints of the cloud platform’s resiliency boundaries like we do on AWS; we can design our subnets to match our workload’s needs. In this case, we need a Subnet for our solution’s front end, which hosts the ASP.NET Core Blazor Web Application, and we need a Subnet for our solution’s Backend, which hosts the ASP.NET Core Web API. Whether we choose to provision Virtual Machines in a Regional manner, taking advantage of Azure’s Fault Domains, or in a Zonal manner—taking advantage of Azure’s Availability Zones does not affect the network design. Both options are available to us when we decide to provision Virtual Machines.

## Network Routing

In Chapter 7, when we set up this solution on AWS, we needed to configure an Internet Gateway, NAT Gateways, and properly configured Route Tables in order for our Virtual Machines to have outbound access to the Internet. On Azure, we don’t need to configure equivalent components because Azure provides a default gateway and automatically configures Virtual Machines to use it. If we wanted to block Internet access or route Internet traffic another way, we would need to configure additional resources.

## Load Balancing

When discussing Load Balancers as a component of our architecture, we will inevitably use some well-established and familiar terms, but we will be using them in a different context. This can be confusing. Therefore, I hope to tackle the elephant in the room. Our solution has a frontend—the web application that serves up web pages for the end user’s web browser. Our solution also has a Backend—the REST Web API that our Web Application calls to talk to the database and perform stateful operations. Our solution will also leverage two Load Balancers: one to distribute load across our Frontend Web Servers running the Web Application and another to distribute load across our Backend Web Servers running the Web API. 

![Resource][image-3]
_Too many "Frontends" and "Backends"_

Within the context of each load balancer, each load balancer will have its own front end and back end. It’s important to note the context when using these terms, as the front end of our solution refers to a different architectural component at an altogether different architectural granularity. We need to understand that when we refer to the Frontend of our solution, we are talking about all of the components that make up the Frontend of our solution function properly, and when we are talking about the Frontend of the “Frontend” Load Balancer we are talking about the networking endpoint that accepts traffic for the “Frontend” of our solution.

In Chapter 7, when we setup this solution on AWS we used the AWS Application Load Balancer service. On Azure, we’ll use the Azure Load Balancer service. Both services function very similarly but they are structured a little differently and use different terminology to describe similar concepts.


| AWS                             | Azure                     | Description                                                                                                            |
| ------------------------------- | ------------------------- | ---------------------------------------------------------------------------------------------------------------------- |
| Application Load Balancer (ALB) | Azure Load Balancer       | Load Balancer                                                                                                          |
| Listener                        | Frontend IP Configuration | The singular endpoint that accepts incoming traffic on a Load Balancer                                                 |
| Target Group                    | Backend Address Pool      | A collection of Virtual Machines that incoming traffic is forwarded to                                                 |
| Health Check                    | Health Probe              | An endpoint published by each of the backend Virtual Machines that indicates it is healthy and ready to handle traffic |

_Mapping of synonymous Load Balancer components between AWS and Azure_

As we discussed in Chapter 4, a Load Balancer essentially provides a singular Frontend Endpoint and distributes network traffic across a multitude of Backend Virtual Machines. On AWS, while they call this Frontend Endpoint a “Listener”, on Azure, it is called the “Frontend IP Configuration”. Likewise, the Backend Virtual Machines are called the “Target Group” in AWS, and they are called the “Backend Address Pool” on Azure.

![Resource][image-4]
_Isolated Subnets for Frontend and Backend Application Components_

The Azure Load Balancer uses rules to determine how incoming traffic is routed to backend pools.

The Azure Load Balancer organizes how it routes incoming traffic using rules. Each rule has a protocol, a frontend component, and a backend component. The rule’s frontend component configures where and how the network traffic should come into the Load Balancer. This includes a port to expose, which Frontend IP Configuration to expose the port on, and what Health Probe it should use to determine which backend nodes are healthy and ready to receive traffic. The backend component of the rule specifies which Backend Address Pool to route traffic to and what port to use. 

The Health Probe is configured with its own protocol, port, and request path. This endpoint is hit regularly by the Load Balancer on each of the Virtual Machines within the Backend Address Pool to verify they are healthy and ready to receive traffic. Because our Application isn’t changing whether we deploy to AWS or Azure, the Frontend of our solution—the web application—will continue to use the root path `/` and the Backend—the REST Web API—will continue to use the custom health check endpoint we setup at `/health`. 

## Network Security

In Chapter 7 we setup four Security Groups in AWS for each logical stop that network traffic makes within our solution architecture. In Azure, we only need two Security Groups because the Azure Load Balancer is automatically granted access to our Virtual Machines using the Rules that we configured in it.

![Resource][image-5]
_Frontend Node Pool network traffic flow_

From the perspective of virtual machines handling traffic within the front end, they will receive traffic on port 5000 using the HTTP protocol. The C# application will make requests to the REST Web API hosted in the backend, but we’ll be routing all our requests to the backend through the backend load balancer on port 80 using the HTTP protocol. On Azure, we don’t need to explicitly allow this egress traffic within the network.

![Resource][image-6]
_Backend Node Pool network traffic flow_

From the perspective of the Virtual Machines handling traffic within the Backend, they will be receiving traffic on port 5000 using the HTTP protocol. The C# application code will be making requests to the PostgreSQL Database on port 5432 using the HTTPS protocol. On Azure, we don’t need to explicitly allow this egress traffic within the network.

## Secrets Management

Secrets such as database credentials or service access keys need to be stored securely. Each cloud platform has its own service that provides this functionality. On Azure, this service is called Azure KeyVault.

| AWS             | Azure                            | Description                                                                  |
| --------------- | -------------------------------- | ---------------------------------------------------------------------------- |
| IAM             | Microsoft Entra                  | Identity Provider                                                            |
| Secrets Manager | KeyVault                         | Secure secret storage                                                        |
| IAM Role        | User Assigned Managed Identity   | Identity for machine to machine interaction                                  |
| IAM Policy      | Role Based Access Control (RBAC) | Permissions to perform specific operations on specific services or resources |
| IAM Role Policy | Role Assignment                  | Association of specific permissions to specific identites                    |

_Mapping of synonymous Identity & Access Management components between AWS and Azure_

Secrets stored in Azure KeyVault can be accessed by Virtual Machines once they have the necessary Role Based Access Controls granted. In Chapter 7, we used an AWS IAM Role assignment to allow a Virtual Machine to do this. Azure works similarly by attaching one or more User Assigned Managed Identities to the Virtual Machines and then creating Role Assignments for the Managed Identities to have specific Roles that grant the necessary permissions.

![Resource][image-7]
_KeyVault Architecture_

Granting the Managed Identity that is attached to the Virtual Machines access to the “Key Vault Secrets User” Role will allow the Virtual Machines to read the secret values from Key Vault. This does not put the secrets on the machine. The Virtual Machine will need to use the Azure CLI to access the KeyVault secrets.

## Virtual Machines

Now that we have everything we need for our solution, we can finish by talking about where our application components will actually run: Virtual Machines provisioned on Azure’s Virtual Machine service. When provisioning Virtual Machines on Azure, you have two options. First, you can provision static virtual machines. In this approach, you need to specify key characteristics for every Virtual Machine. The second option is to provision a Virtual Machine Scale Set. This will allow you to dynamically scale up and down based on demand as well as auto-heal Virtual Machines that fail.

| AWS                      | Azure                            | Description                                                                                                               |
| ------------------------ | -------------------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| EC2                      | Virtual Machines                 | Virtual Machine Service                                                                                                   |
| AMI                      | Virtual Machine Image            | Virtual Machine Image either from Marketplace or custom build (e.g. using tools like Packer)                              |
| IAM Role                 | User Assigned Managed Identity   | Identity for machine to machine interaction                                                                               |
| Auto-Scaling Group (ASG) | Virtual Machine Scale Set (VMSS) | Set of dynamically provisioned Virtual Machines that can be scaled up/down using a Virtual Machine configuration template |
| Launch Template          | Virtual Machine Profile          | Configuration template used to create new Virtual Machines                                                                |

_Mapping of synonymous Virtual Machine service components between AWS and Azure_

In Chapter 7, we provisioned our solution using AWS Elastic Cloud Compute (EC2) service. Azure Virtual Machines share a similar structure to EC2 instances. Like on AWS, Azure Virtual Machines are connected to their corresponding Subnet by way of a virtual Network Interface. However, on Azure, we have two types of Network Security rules: Network Security Groups (NSGs) and Application Security Groups (ASGs). While both are used to control traffic on Azure, NSGs focus on specifying lower-level network rules such as port and protocol filtering for network-level resources defined as IP Address ranges. AGSs, on the other hand, provide a higher level of abstraction that allows you to group resources based on the role they play within the application. 

![Resource][image-8]
_Azure Virtual Machine Architecture_

Alternatively, you can use an Azure Virtual Machine Scale Set (VMSS) to dynamically provision and manage the Virtual Machines. In this approach, you provide the VMSS with some configuration and parameters on when to scale up and when to scale down, and the VMSS will take care of everything else.

![Resource][image-9]
_Azure Virtual Machine Scale Set Architecture_

Azure Virtual Machine Scale Sets allow you to provide fine grained configuration for each of the Virtual Machines that it will spin up on your behalf but it also provides a set of policies that allow you to control behavior of the VMSS relating to when instances fail unexpectedly, when Azure needs to update them, or whether to scale up or down the number of Virtual Machines.

[image-1]:	../images/Azure-VirtualNetwork.png
[image-2]:	../images/Azure-VirtualNetwork-FrontendBackend.png
[image-3]:	../images/Azure-LoadBalancer-FrontendBackend.png
[image-4]:	../images/Azure-LoadBalancer.png
[image-5]:	../images/Azure-NSG-FrontEnd.png
[image-6]:	../images/Azure-NSG-BackEnd.png
[image-7]:	../images/Azure-KeyVault-Overview.png
[image-8]:	../images/Azure-Compute-VirtualMachines.png
[image-9]:	../images/Azure-Compute-VMSS.png