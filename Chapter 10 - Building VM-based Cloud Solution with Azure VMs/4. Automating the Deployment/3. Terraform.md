# 3. Terraform

With both of our Virtual Machine Images built and their versions input into our `tfvars` file, our Terraform automation pipeline is ready to take the reigns and not only provision our environment but deploy our solution as well (although not technically). The deployment was technically done within the packer build process, with the physical deployment packages being copied to the home directory and the Linux service setup primed and ready. Terraform is finishing the job by actually launching Virtual Machines using these images.

In Chapter 7, we went into great detail about each step of the GitHub Actions Workflow that executes Terraform to provision the Cloud Environment and deploy the Application Code. Thanks to the nature of Terraform’s cloud-agnostic architecture, this overwhelmingly stays the same. The only thing that changes is the final step where we execute Terraform.

Just like we did in Chapter 7 with the AWS provider, we can set the authentication context using environment variables that are specific to the `azurerm` provider. In this case, the four Azure credentials attributes are passed in with the following environment variables:

- **Tenant ID:** `ARM_TENANT_ID`
- **Subscription ID:** `ARM_SUBSCRIPTION_ID`
- **Client ID:** `ARM_CLIENT_ID`
- **Client Secret:** `ARM_CLIENT_SECRET`

Just like we did in Chapter 7 with the AWS provider, we need to configure the Azure-specific backend that stores Terraform state using the `-backend-config` command line arguments to the `terraform init` command. Unlike AWS, which only specifies an S3 Bucket Name to configure the backend to save Terraform State to S3—in order to configure the Azure backend, we need to specify three fields to triangulate a location in Azure Blob Storage to save Terraform State—Resource Group, Storage Account, Blob Storage Container.

The hierarchy of Azure resources looks like this:

- Resource Group
	- Storage Account
		- Blob Storage Container
			- Terraform State Files

Like with the AWS provider, the backend uses a “Key” and the Terraform Workspace name to uniquely identify the location to store state files.

	- id: apply
	  name: Terraform Apply
	  env:
	    ARM_SUBSCRIPTION_ID: ${{ vars.ARM_SUBSCRIPTION_ID }}
	    ARM_TENANT_ID: ${{ vars.ARM_TENANT_ID }}
	    ARM_CLIENT_ID: ${{ vars.TERRAFORM_ARM_CLIENT_ID }}
	    ARM_CLIENT_SECRET: ${{ secrets.TERRAFORM_ARM_CLIENT_SECRET }}
	    BACKEND_RESOURCE_GROUP_NAME: ${{ vars.BACKEND_RESOURCE_GROUP_NAME }}
	    BACKEND_STORAGE_ACCOUNT_NAME: ${{ vars.BACKEND_STORAGE_ACCOUNT_NAME }}
	    BACKEND_STORAGE_CONTAINER_NAME: ${{ vars.BACKEND_STORAGE_CONTAINER_NAME }}
	    TF_BACKEND_KEY: ${{ env.APPLICATION_NAME }}-${{ env.ENVIRONMENT_NAME }}
	  working-directory: ${{ env.WORKING_DIRECTORY }}
	  run: |
	    terraform init \
	      -backend-config="resource_group_name=$BACKEND_RESOURCE_GROUP_NAME" \
	      -backend-config="storage_account_name=$BACKEND_STORAGE_ACCOUNT_NAME" \
	      -backend-config="container_name=$BACKEND_STORAGE_CONTAINER_NAME" \
	      -backend-config="key=$TF_BACKEND_KEY"
	
	    terraform apply -auto-approve

Notice how, unlike with the AWS solution, we don’t need to perform a targeted `terraform apply`. This is because we don’t need to do dynamic calculation based on the number of Availability Zones in region to configure our Virtual Network. This is due to Azure Virtual Network and their Subnets spanning all Availability Zones within the Region whereas, on AWS, a Subnet is constrained to a specific Availability Zone within the parent Virtual Network’s Region.

These subtle architectural differences between the cloud platforms can create radical structural changes even when deploying the exact same solution using the exact same technologies. It is a sobering reminder that while knowledge of the core concepts we looked at in Chapters 4 through 6 will help us transcend to a multi-cloud point-of-view, in order to implement practical solutions, we need to understand the subtle nuances of each platform.