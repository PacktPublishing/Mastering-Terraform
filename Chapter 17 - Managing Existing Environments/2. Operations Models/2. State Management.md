# 2. State Management

When starting to manage long-lived environments using Terraform, whether they are just for development, testing, or actual production workloads, the foundational change to your operating model is the introduction of Terraform State. We discussed Terraform State in the first chapter of this book when we delved into Terraform’s architecture, so we already know the value it brings as the arbiter of what the environment should look like, but creating state and managing it is part of the day-to-day operations of managing environments with Terraform.

## Just Say No to Manual State Manipulation
As we have established, Terraform State files are essentially just JSON text files that contain an inventory of the resources as they were provisioned during the last Terraform Apply. It might be tempting to manually manipulate the state file just by opening it up in a text editor and changing things around. However, this is ill-advised. The Terraform command line interface has many commands that provide a safe way to perform state manipulation operations, and HashiCorp is even starting to aggressively roll out HashiCorp Configuration Language features to enable state manipulation through the codebase itself rather than bespoke administrator tinkering through the CLI. Besides transitioning from an imperative approach to a declarative one, it also has the added benefit for module authors to make version upgrade paths more seamless by building a safe way to update without costly blue-green deployments or implementing short-term “fixes” that become long-term problems.

## Access Control

Due to Terraform’s nature of being this extensible chameleon of an infrastructure-as-code tool that adapts to each target platform through its provider plugins, it also adapts to the target platforms by way of the backend provider that is used. By default, Terraform uses the local filesystem for the state, but this, of course, is not used when managing long-lived environments. As we discussed, when we implemented the solution across each of the three cloud platforms covered in this book, we used a different backend provider to store the state on the corresponding cloud. 

In AWS, we used the `s3` backend, which stored our state on AWS’s Simple Storage Service (S3). By default, only users with sufficient Identity & Access Management permissions are able to access the data. This allows us to have fine-grained control over the users (and the machines) who can access the state files. Likewise, on Azure, when we used the `azurerm` backend, and on Google Cloud Platform, when we used the `gcs` backend, Terraform stored the state files on the corresponding storage service for each of these cloud platforms. Like AWS, the other cloud platforms implement similar access controls to prevent unprivileged access. On Azure, this takes the form of Azure Role-Based Access Controls (RBACs) specified at either the Resource Group or the Subscription level. On Google Cloud, this takes the form of Access Control Lists that are driven at the project level.

## Encryption

In addition to identity-based access controls that we can apply on the cloud services that are hosting our Terraform State files we can also employ built-in capabilities of these services to leverage various levels of encryption. The most simplest level is the built-in transparent data encryption that protects us if the cloud provider has a physical data breach. This is a nice insurance policy but it’s one of the more unlikely attack vectors.

The more likely way our Terraform State files will become vulnerable is if we have a leaky identity and access management controls. One method for adding an additional layer of security is by leveraging encryption of the data within the storage service itself. When you do this, access to the files is not enough; you need access to the encryption keys themselves. 

On AWS, this is done using AWS Key Management Service (KMS), which allows you to create and manage your own keys that can be used to encrypt your Terraform State Files. Similar capabilities exist on both Azure and Google Cloud. On Azure, you would employ Customer-Managed Keys created in Azure KeyVault, and on Google, you would employ the same approach but, of course, use the equivalent Google Cloud Service called Google Cloud KMS. If you want a cloud-agnostic approach, you could leverage a multi-cloud Key Management solution like HashiCorp Vault.

## Backup

In the previous chapter, we looked at how to import an existing environment into Terraform and saw that even while there are built-in tools to do this, it can be tedious and error-prone. Unfortunately, the only thing keeping your environment in the classification of environments “managed by Terraform” is the state file. If you lose your state file or if it becomes corrupted or out of sync beyond all recollection, your clean environment that was provisioned by Terraform could very easily become an orphaned environment, no longer managed by Terraform and requiring you to consider your options when it comes to importing or re-provisioning.

Don’t let that happen! You should keep backups of your state files. Most of the Terraform Backends that we looked at support this out of the box in several different ways. First by enabling version tracking so you actually have a versioned history of the state file within the storage service itself. This is a very convenient and cost effective way to help you overcome small issues like human error or transient deployment failures. 

However, you should also consider more advanced cross-region replication features of the cloud storage service hosting your Terraform State Backend to help you in case of a broader outage. Terraform State going temporarily offline or unavailable doesn’t impact your solution’s availability, but it does impact your ability to exert control in the environment in the case of an outage. So, it’s important to think about implementing cross-region replication and a backup strategy to ensure all scenarios are covered.

## Organization

One of the easiest things you can control is where your Terraform workspaces are stored. It doesn’t take a whole bunch of bells and whistles to protect your state files if you properly segment your Terraform workspaces and work within the security boundaries that your cloud has to offer. 

On AWS, you may want to create more S3 buckets and place those buckets in different AWS accounts to ensure there isn’t secret leakage due to overly benevolent IAM policies. 

Likewise, on Azure, more Storage Accounts can be provisioned and placed in Azure subscriptions to isolate them more effectively against overly generous Subscription-level permissions.

On Google Cloud, consider carefully what project the Google Cloud Storage service should be provisioned within and opt for an isolated Project for Terraform State. This will ensure that the application and its administrators don’t necessarily have access to the secrets that may be in the Terraform State File.