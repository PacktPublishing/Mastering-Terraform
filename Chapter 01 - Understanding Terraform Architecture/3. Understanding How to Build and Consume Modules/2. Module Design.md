# 2. Module Design

In many ways, the decision to create a module in Terraform is the same as deciding to write a new method when writing in a traditional programming language like Java or C#.

Just like in a traditional programming language, you could write all your code from start to finish in a single file using a single method, and if there were repeated parts, you would copy and paste them to repeat them.

Just like in a traditional programming language, there are reasons to write methods encapsulating repeating blocks of code. Otherwise, if you didn’t encapsulate that code into a method, you must copy and paste it repeatedly.

The decision about when to create a module versus just putting it in the root module is an important one. You should have good reasons for creating a module. You should always focus on value. When someone uses your module—which could be just yourself or your team—does it make their life easier by using it?

## Root Modules

There are many different ways to set up your root module in Terraform. The debate continues, with some vehemently advocating one method over the other. It’s important to be aware of the different approaches so that you can recognize them when you see them and evaluate which approach works best for you.

### Folder Per Environment

One common technique for structuring a root module is setting up a different folder for each environment you want to provision and maintain. In this approach, there is a folder for each long-lived environment. This folder contains a root module that can stand alone from the other environments. Consider the folder structure below:

- `/terraform`
	- `/dev`
		- `main.tf`
		- `versions.tf`
		- `variables.tf`
		- `terraform.tfvars`
	- `/test`
		- `main.tf`
		- `versions.tf`
		- `variables.tf`
		- `terraform.tfvars`
	- `/prod`
		- `main.tf`
		- `versions.tf`
		- `variables.tf`
		- `terraform.tfvars`

The above folder structure has three environments: `dev`, `test`, and `prod`. Each environment has its own root module is completely isolated from other modules. It has its own `required_providers` block and defines its own provider declarations. This approach has strong isolation between each environment—so much so that practically every aspect of the deployment could be altered from environment to environment. The version of Terraform, the version of the providers, and the version of the other modules used within the solution, the input parameters, and their values are all customized within the files within the corresponding folder for the environment.

This approach is more common where the practitioners aren’t comfortable using GitFlow and maintaining other branches and following a software development lifecycle where infrastructure updates are promoted from less mature level branches (e.g.,`develop`) to more mature branches (e.g., `main`—where production code exists). 

### Variable File Per Environment

Another technique is to maintain a single Terraform codebase and multiple input variable files for each environment. This approach is focused on maintaining consistency and compatibility between environments. It is more difficult with this approach to make massive structural differences between the environments as it becomes difficult to merge changes from branch to branch.

Consider the folder structure below:

- `/terraform`
	- `/modules`
		- `/solution`
			- `main.tf`
			- `versions.tf`
			- `variables.tf`
	- `/env`
		- `dev.tfvars`
		- `test.tfvars`
		- `prod.tfvars`
	- `main.tf`
	- `versions.tf`
	- `variables.tf`
	- `terraform.tfvars`

Like the previous approach, where we had explicit folders for each environment, this approach still allows the same variation between environments but requires you to maintain long-lived branches for each environment as you make changes to the core structure of the root module. This aligns more with a software development process called GitFlow (more on that in a future chapter). 

The key characteristics of this approach are that environmental differences are captured in different input variable values stored in the corresponding `*.tfvars` files (more on this in a future chapter). The goal is that any variation between the environments will eventually be stored within these files, and the codebases for each environment—stored within several long-lived source code branches—will eventually mirror each other. This allows us to have different sizes and counts in our production environment vs. our development environment and maintain consistency between the architecture and configuration deployed across each environment.

## Reusable Modules

Now that we have our root module under control, it’s time to start thinking about when to create reusable modules that can be utilized in our root modules to produce the sophisticated cloud architectures that will power our applications and solutions.

### Encapsulation of Complexity

The number of resources you plan to encapsulate within the module is an important metric, as it can indicate if you are reducing complexity by creating a module or adding more (spoiler alert: adding more is bad). Modules can range from one Resource to dozens—even hundreds—of resources. When considering the number of resources you put into your module, you should consider the value you bring when someone uses the module. 

If your module only encapsulates one resource block, your code would likely be simpler by directly referencing the Resource. In this situation, the module adds a layer of abstraction on top of the underlying Resource you are provisioning. If that's all it's doing, then you need to reduce the complexity more to justify the creation of a module.

Suppose your module encapsulates a few tightly related resources that are highly dependent on each other and have limited integration points with other resources. For example, when creating a Network Security Group and a collection of rules. Creating a module encapsulating these tightly coupled resources might be a good idea because it will make it easier and more concise for the developer to create a Network Security Group. In that case, this is the sweet spot for creating a module. You are likely trading one or two additional input variables for one or two additional corresponding resource blocks. That's a good trade. 

![Resource][image-1]
_Module Design - Encapsulation of Complexity_

The above diagram shows that this module is provisioning three resource types. Our module defines a single interface that will provision this cluster of resources. Some simple inputs, `A` and `B`, are passed to the `Main Resource` and `Child Resource 1`. A more complex input object, `C`, which happens to be an array, is passed in and used to construct a resource block for each item in the list.

### Repeating Patterns
Another common scenario is when you have many resources that you want to be repeated based on the size of a collection (either a list or a map). In this situation, you should tell each Resource how many copies of it you want and pass in all the input variables to satisfy its requirements. 

![Resource][image-2]
_Module Design - Repeating Inside the Module_

However, if you encapsulate the repeating resources into a module, rather than repeating every Resource, you repeat the module. This approach can significantly enhance the readability and maintainability of your code.

![Resource][image-3]
_Module Design - Repeating Outside the Module_

The outside consumer of the module is responsible for introducing iteration on the module resource itself.

```
	module "foo" {
	  source = "../modules/rando"
	  count  = 5
	}
```

By applying the iterator to the module itself, we achieve the same outcome as if we adorn every Resource declared in the module with a count and pass in the number of resources as an input variable to the module. However, working with every Resource inside the module becomes more difficult.

When you design your module to be repeated by a parent module, your module doesn't have to think about the complexity of how many resources the parent module wants to create across all items in the collection. Each module instance only has to worry about one instance of each Resource. 

Does it flatten or simplify the Resource in a way that can make the Resource easier to use?

If you are starting from scratch, it's best to let those patterns emerge over time. And the code in the method is, by its very nature, a rather opinionated piece of code. Once you identify one, all it takes is a destroy, refractor, and re-apply, and you're using your new module.

Destroying the entire environment and starting over isn't always an option. This approach can only be used in a development or testing environment. In production environments, you will need to take a different approach.

Sometimes, you can write a method you can use in many scenarios. This approach is most common when developing Framework code that tackles a horizontal problem space. But sometimes methods are intended to do very particular things. 

This same principle applies to Terraform module design. Some modules are highly flexible and designed in a framework, while others are more like “Hey, I want to do this specific thing, and I want to keep it simple.” With a Scenario-Driven Module, the interface to the module will be very, very simple because it’s only about shepherding dependency inputs into the module’s scope that the module needs and doesn't have on its own within its scope. 

A Framework Module typically has a much more complex interface; as a result, it will have many more levers that the module consumer can pull. Sometimes, those inputs are no longer straightforward primitive types (string, bool, number); they are complex objects you construct and pass in. As the number of scenarios your module supports increases, so does the complexity of your module. You have to pass in a lot more parameters to configure it. It will become much more tedious and error-prone to pass those complex objects as you may have to implement more object construction logic using local variables. 

Most Terraform Providers have Resources that do not require you to construct complex objects to use them. You will use primitive types, sometimes collection types, and nested blocks. 

However, when building modules, you do have the ability to create complex objects as input variables. You should avoid overly complex data structures because of the complexity that it adds. Frequently, the dependencies between resources are relatively small. So, if you only need small pathways to connect two objects, why create massive Data Transfer Objects (DTOs) to pass context from one object to another? It makes the code easier to understand and easier to maintain. Future authors and your module consumers will be cursing your name just like in poorly written traditional software. 

I've seen software where there have been methods where instead of using the correct primitive types, bool, number, everything is a string. Will that work? Sure. But does that make it easy to understand? Does that inject additional complexity? Like constantly type-casting the input values back and forth between strings into their proper type. You should use the correct type and simplify the interface.

We have to strike a balance between using complex types and having too many input variables on a module because having too many input variables affects the cyclomatic complexity, making it difficult to maintain. However, unlike other languages, working with HashiCorp Configuration Language (HCL) is challenging when using complex objects. Developers could be more efficient when constructing and transforming large, complex data types. HCL is excellent for developers when declaring and associating resources by piping output variables into input variables.

[image-1]:	../images/Modules-Encapsulation.png
[image-2]:	../images/Modules-Repeating.png
[image-3]:	../images/Modules-Repeating-Simplified.png